{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final ML: Clustering y Semi-Supervised Learning\n",
    "## An√°lisis de Cooperativas del Segmento 1 en Ecuador\n",
    "\n",
    "---\n",
    "\n",
    "**Curso:** Machine Learning\n",
    "\n",
    "**Objetivo:** Agrupar cooperativas de ahorro y cr√©dito seg√∫n caracter√≠sticas financieras y validar coherencia de clusters contra ratings reales.\n",
    "\n",
    "**Fechas:** Noviembre 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Tabla de Contenidos\n",
    "\n",
    "1. **Setup e Instalaci√≥n** - Configuraci√≥n inicial\n",
    "2. **Parte 1: Obtenci√≥n de Datos** - Web scraping y extracci√≥n\n",
    "3. **Parte 2: An√°lisis Exploratorio (EDA)** - Exploraci√≥n de datos\n",
    "4. **Parte 3: Clustering No Supervisado** - K-Means, Agglomerative, DBSCAN\n",
    "5. **Parte 4: Semi-Supervised Learning** - Label Propagation, Self-Training\n",
    "6. **Resultados y Conclusiones** - An√°lisis final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ SETUP E INSTALACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup para Google Colab (comentar si es local)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üì± Ejecut√°ndose en Google Colab\")\n",
    "    \n",
    "    # Clonar repositorio\n",
    "    !git clone https://github.com/jjjulianleon/ProyectoFinalML.git\n",
    "    %cd ProyectoFinalML\n",
    "    \n",
    "    # Instalar dependencias\n",
    "    !pip install -q -r requirements.txt\n",
    "    \n",
    "    print(\"‚úì Dependencias instaladas\")\n",
    "else:\n",
    "    print(\"üíª Ejecut√°ndose localmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar estilo\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Agregar src al path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Imports de m√≥dulos locales\n",
    "from etl.generate_sample_data import generate_sample_cooperativas_data\n",
    "from models.clustering import ClusteringAnalyzer\n",
    "from models.semi_supervised import SemiSupervisedLearner\n",
    "\n",
    "print(\"‚úì Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Crear directorio para figuras\n",
    "Path('figures').mkdir(exist_ok=True)\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuraci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2Ô∏è‚É£ PARTE 1: OBTENCI√ìN Y PREPARACI√ìN DE DATOS\n\n**EXTRACCI√ìN AUTOM√ÅTICA 100%:**\n- Descarga autom√°tica de PDFs desde URLs\n- Extracci√≥n de texto con pdfplumber\n- Procesamiento con OpenAI API (LLM)\n- Generaci√≥n de dataset estructurado"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ‚ö†Ô∏è  REQUISITO: EXTRACCI√ìN AUTOM√ÅTICA 100% CON DATOS REALES\n# Este notebook REQUIERE datos reales extra√≠dos mediante web scraping con OpenAI API\n# NO utiliza datos de ejemplo/prueba\n\nprint(\"=\"*70)\nprint(\"üöÄ EJECUTANDO PIPELINE ETL - EXTRACCI√ìN AUTOM√ÅTICA DE DATOS REALES\")\nprint(\"=\"*70)\nprint(\"\\n‚ö†Ô∏è  REQUISITO IMPORTANTE:\")\nprint(\"   Este an√°lisis REQUIERE datos reales extra√≠dos de PDFs\")\nprint(\"   Se usar√° web scraping autom√°tico con OpenAI API\")\nprint(\"   NO se usar√°n datos de ejemplo/prueba\\n\")\n\n# Detectar si estamos en Google Colab\nimport sys\nimport os\nfrom getpass import getpass\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"üì± Ejecut√°ndose en Google Colab\")\n    print(\"\\nüîë CONFIGURACI√ìN DE API KEY\")\n    print(\"=\"*70)\n    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n    print(\"Obt√©n una en: https://platform.openai.com/api-keys\\n\")\n    print(\"Recomendado: Usar Google Colab Secrets\")\n    print(\"  1. Click en üîë (llave) en panel izquierdo\")\n    print(\"  2. Agregar secreto: OPENAI_API_KEY\")\n    print(\"  3. Pegar tu API key\")\n    print(\"=\"*70 + \"\\n\")\n    \n    # Intentar obtener de Colab Secrets\n    api_key = None\n    try:\n        from google.colab import userdata\n        api_key = userdata.get('OPENAI_API_KEY')\n        if api_key:\n            print(\"‚úì API Key obtenida de Google Colab Secrets\\n\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  No se pudo acceder a Colab Secrets\\n\")\n    \n    # Si no est√° en secrets, pedir al usuario\n    if not api_key:\n        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n        print()\n    \n    # Validar que se proporcion√≥ API key\n    if not api_key or api_key.strip() == \"\":\n        print(\"‚ùå ERROR: API Key es requerida para extraer datos reales\")\n        print(\"   No se puede continuar sin API key\")\n        raise ValueError(\"API Key no proporcionada - Extracci√≥n de datos reales es OBLIGATORIA\")\n    \n    os.environ['OPENAI_API_KEY'] = api_key\n    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n    \nelse:\n    print(\"üíª Ejecut√°ndose localmente\")\n    print(\"\\nüîë CONFIGURACI√ìN DE API KEY\")\n    print(\"=\"*70)\n    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n    print(\"Obt√©n una en: https://platform.openai.com/api-keys\\n\")\n    \n    # Intentar obtener de .env\n    from dotenv import load_dotenv\n    load_dotenv(override=True)\n    \n    api_key = os.getenv('OPENAI_API_KEY')\n    \n    if not api_key:\n        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n        print()\n    \n    # Validar que se proporcion√≥ API key\n    if not api_key or api_key.strip() == \"\":\n        print(\"‚ùå ERROR: API Key es requerida para extraer datos reales\")\n        print(\"   Configura OPENAI_API_KEY en .env o proporciona la key cuando se solicite\")\n        raise ValueError(\"API Key no proporcionada - Extracci√≥n de datos reales es OBLIGATORIA\")\n    \n    os.environ['OPENAI_API_KEY'] = api_key\n    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n\n# Ejecutar pipeline ETL (OBLIGATORIO - sin fallback)\nprint(\"=\"*70)\nprint(\"üì• INICIANDO EXTRACCI√ìN DE DATOS REALES\")\nprint(\"=\"*70 + \"\\n\")\n\ntry:\n    from etl.run_etl_pipeline import run_etl_pipeline\n    \n    # Ejecutar pipeline (descarga + extracci√≥n con OpenAI)\n    df = run_etl_pipeline(\n        urls_file=\"data/cooperativas_urls.txt\",\n        output_csv=\"data/processed/cooperativas_data.csv\",\n        download_dir=\"data/raw\"\n    )\n    \n    # Validar resultados\n    if df is None or df.empty:\n        print(\"\\n‚ùå ERROR CR√çTICO: No se extrajeron datos\")\n        print(\"   ‚Ä¢ Verifica que los URLs en data/cooperativas_urls.txt sean v√°lidos\")\n        print(\"   ‚Ä¢ Verifica que tu API key de OpenAI sea v√°lida\")\n        print(\"   ‚Ä¢ Verifica tu conexi√≥n a internet\")\n        print(\"   ‚Ä¢ Intenta ejecutar nuevamente\")\n        raise ValueError(\"Extracci√≥n de datos fall√≥ - No hay datos para analizar\")\n    \n    print(f\"\\n‚úÖ √âXITO: {len(df)} muestras extra√≠das correctamente\\n\")\n    print(f\"üìä Distribuci√≥n de Ratings:\")\n    print(df['rating'].value_counts().sort_index())\n\nexcept Exception as e:\n    print(\"\\n‚ùå ERROR EN EXTRACCI√ìN DE DATOS REALES:\")\n    print(f\"   {str(e)}\\n\")\n    print(\"Acciones recomendadas:\")\n    print(\"   1. Verifica que tu API key sea v√°lida\")\n    print(\"   2. Verifica que los URLs en data/cooperativas_urls.txt sean accesibles\")\n    print(\"   3. Verifica tu conexi√≥n a internet\")\n    print(\"   4. Aseg√∫rate de que tienes cr√©dito en OpenAI\")\n    print(\"   5. Intenta ejecutar nuevamente\\n\")\n    raise\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ DATOS REALES CARGADOS Y LISTOS PARA AN√ÅLISIS\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccionar datos\n",
    "print(\"üìã Primeras filas del dataset:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\nüìä Informaci√≥n del dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas\n",
    "print(\"üìà Estad√≠sticas Descriptivas:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(\"\\n‚ùì Valores Faltantes:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datos procesados\n",
    "df.to_csv('data/processed/cooperativas_data.csv', index=False)\n",
    "print(\"‚úì Datos guardados en: data/processed/cooperativas_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ PARTE 2: AN√ÅLISIS EXPLORATORIO (EDA)\n",
    "\n",
    "Exploraci√≥n no supervisada de los datos para identificar patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar variables num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Variables num√©ricas a analizar ({len(numeric_cols)}):\")\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci√≥n por rating\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    for rating in df['rating'].unique():\n",
    "        data = df[df['rating'] == rating][col]\n",
    "        axes[idx].hist(data, alpha=0.5, label=f'Rating {rating}')\n",
    "    \n",
    "    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Valor')\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "    axes[idx].legend(fontsize=8)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/01_distribucion_por_rating.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/01_distribucion_por_rating.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlaci√≥n\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlaci√≥n - Indicadores Financieros', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_matriz_correlacion.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/02_matriz_correlacion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de correlaciones altas\n",
    "print(\"üîó Correlaciones m√°s altas (excluyendo diagonal):\")\n",
    "corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append({\n",
    "            'variable1': corr_matrix.columns[i],\n",
    "            'variable2': corr_matrix.columns[j],\n",
    "            'correlacion': corr_matrix.iloc[i, j]\n",
    "        })\n",
    "\n",
    "corr_pairs_df = pd.DataFrame(corr_pairs).sort_values('correlacion', ascending=False, key=abs)\n",
    "display(corr_pairs_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reducci√≥n dimensional con t-SNE (o PCA como fallback)\nprint(\"üîÑ Aplicando reducci√≥n dimensional para visualizaci√≥n...\")\n\n# Normalizar datos\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[numeric_cols])\n\nn_samples = X_scaled.shape[0]\nn_features = X_scaled.shape[1]\n\nprint(f\"üìä Par√°metros de reducci√≥n:\")\nprint(f\"  ‚Ä¢ N√∫mero de muestras: {n_samples}\")\nprint(f\"  ‚Ä¢ N√∫mero de features: {n_features}\")\n\n# CR√çTICO: Validar que tenemos suficientes muestras para reducci√≥n dimensional\nif n_samples < 2:\n    print(f\"\\n‚ùå ERROR CR√çTICO: Dataset insuficiente para visualizaci√≥n\")\n    print(f\"   Solo tienes {n_samples} muestra(s)\")\n    print(f\"   Se necesitan m√≠nimo 2 muestras para cualquier reducci√≥n dimensional\")\n    print(f\"\\nACCIONES:\")\n    print(f\"   1. Verifica que data/cooperativas_urls.txt tenga URLs v√°lidos\")\n    print(f\"   2. Aseg√∫rate de que TODOS los PDFs se descargaron exitosamente\")\n    print(f\"   3. Verifica que la extracci√≥n con OpenAI extrae m√∫ltiples filas por PDF\")\n    print(f\"   4. Intenta con m√°s URLs de cooperativas\")\n    raise ValueError(f\"Insuficientes muestras para visualizaci√≥n: {n_samples} < 2\")\n\n# Decidir entre t-SNE y PCA basado en tama√±o del dataset\nif n_samples < 10:\n    print(f\"\\n‚ö†Ô∏è  Dataset muy peque√±o ({n_samples} < 10)\")\n    \n    if n_samples == 2:\n        print(\"   Solo 2 muestras - saltando visualizaci√≥n dimensional\")\n        print(\"   (No se puede reducir dimensionalidad con solo 2 puntos)\")\n        X_reduced = X_scaled  # Usar datos sin reducci√≥n\n        method_name = \"Original (sin reducci√≥n)\"\n    else:\n        print(\"   Usando PCA en lugar de t-SNE (m√°s estable para pocos datos)\")\n        \n        # PCA: m√°ximo n_components = min(n_samples, n_features)\n        max_components = min(n_samples - 1, n_features)\n        n_components = min(2, max_components)  # Intentar 2, pero m√°ximo lo que permite\n        \n        print(f\"   PCA con n_components={n_components} (max permitido: {max_components})\")\n        \n        from sklearn.decomposition import PCA\n        reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n        X_reduced = reducer.fit_transform(X_scaled)\n        method_name = f\"PCA (n_components={n_components})\"\n        \n        print(f\"‚úì PCA completado\")\n        if n_components > 0:\n            print(f\"  Varianza explicada: {reducer.explained_variance_ratio_.sum():.1%}\")\n\nelif n_samples < 15:\n    print(f\"\\n‚ö†Ô∏è  Dataset peque√±o ({n_samples} muestras)\")\n    print(\"   Usando t-SNE con perplexity conservador\")\n    \n    # Para datasets muy peque√±os, usar perplexity muy peque√±o\n    dynamic_perplexity = max(3, min(5, (n_samples - 1) // 3))\n    \n    print(f\"  ‚Ä¢ Perplexity din√°mico: {dynamic_perplexity}\")\n    \n    try:\n        tsne = TSNE(\n            n_components=2, \n            random_state=RANDOM_STATE, \n            n_iter=1000, \n            perplexity=dynamic_perplexity,\n            learning_rate=200,\n            n_iter_without_progress=300\n        )\n        X_reduced = tsne.fit_transform(X_scaled)\n        method_name = \"t-SNE (conservador)\"\n        print(f\"‚úì t-SNE completado\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  t-SNE fall√≥: {e}\")\n        print(\"   Fallback a PCA\")\n        \n        from sklearn.decomposition import PCA\n        max_components = min(n_samples - 1, n_features)\n        n_components = min(2, max_components)\n        \n        reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n        X_reduced = reducer.fit_transform(X_scaled)\n        method_name = f\"PCA (fallback, n_components={n_components})\"\n\nelse:\n    print(\"‚úì Dataset adecuado para t-SNE\")\n    \n    # Para datasets normales, usar perplexity autom√°tico\n    dynamic_perplexity = max(5, min(50, n_samples // 3))\n    \n    print(f\"  ‚Ä¢ Perplexity din√°mico: {dynamic_perplexity}\")\n    \n    tsne = TSNE(\n        n_components=2, \n        random_state=RANDOM_STATE, \n        n_iter=1000, \n        perplexity=dynamic_perplexity\n    )\n    X_reduced = tsne.fit_transform(X_scaled)\n    method_name = \"t-SNE\"\n    print(f\"‚úì t-SNE completado\")\n\n# Visualizar\nplt.figure(figsize=(10, 8))\n\n# Mapear ratings a colores\nrating_to_color = {rating: idx for idx, rating in enumerate(sorted(df['rating'].unique()))}\ncolors = [rating_to_color[r] for r in df['rating']]\n\nscatter = plt.scatter(\n    X_reduced[:, 0] if X_reduced.shape[1] > 0 else [0]*len(df), \n    X_reduced[:, 1] if X_reduced.shape[1] > 1 else [0]*len(df), \n    c=colors,\n    cmap='viridis', \n    s=100, \n    alpha=0.6, \n    edgecolors='black', \n    linewidth=1\n)\n\n# Agregar leyenda con ratings\nfor rating in sorted(df['rating'].unique()):\n    mask = df['rating'] == rating\n    x_data = X_reduced[mask, 0] if X_reduced.shape[1] > 0 else [0]*mask.sum()\n    y_data = X_reduced[mask, 1] if X_reduced.shape[1] > 1 else [0]*mask.sum()\n    \n    plt.scatter(\n        x_data, y_data, \n        label=f'Rating {rating}', \n        s=100, \n        alpha=0.6,\n        edgecolors='black',\n        linewidth=1\n    )\n\nplt.xlabel(f'Dimensi√≥n 1' if X_reduced.shape[1] > 0 else '√çndice', fontsize=11)\nplt.ylabel(f'Dimensi√≥n 2' if X_reduced.shape[1] > 1 else 'Valor', fontsize=11)\nplt.title(f'Visualizaci√≥n con {method_name} - Cooperativas por Rating', fontsize=14, fontweight='bold')\nplt.legend(loc='best')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.savefig('figures/03_tsne_visualization.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"‚úì Gr√°fico guardado: figures/03_tsne_visualization.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ PARTE 3: CLUSTERING NO SUPERVISADO\n",
    "\n",
    "Aplicamos m√∫ltiples algoritmos de clustering y evaluamos su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar analizador de clustering\n",
    "print(\"ü§ñ Inicializando analizador de clustering...\\n\")\n",
    "\n",
    "analyzer = ClusteringAnalyzer(df[numeric_cols], random_state=RANDOM_STATE)\n",
    "X_scaled = analyzer.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Encontrar k √≥ptimo para K-Means con validaci√≥n para datasets peque√±os\nprint(\"üìä Evaluando n√∫mero √≥ptimo de clusters (k)...\\n\")\n\nn_samples = len(df)\n\n# Calcular k_range din√°mico basado en tama√±o del dataset\n# Regla: k debe ser < n/3 para clustering significativo\nmax_k_valid = max(2, n_samples // 3)\n\n# Limitar b√∫squeda basada en tama√±o\nif n_samples < 20:\n    k_range = range(2, min(max_k_valid + 1, 4))  # M√°ximo k=3 para datasets muy peque√±os\n    print(f\"‚ö†Ô∏è  Dataset peque√±o ({n_samples} muestras)\")\n    print(f\"   Limitando b√∫squeda a k ‚àà {list(k_range)}\\n\")\nelif n_samples < 50:\n    k_range = range(2, min(max_k_valid + 1, 6))  # M√°ximo k=5\n    print(f\"‚ö†Ô∏è  Dataset mediano ({n_samples} muestras)\")\n    print(f\"   Limitando b√∫squeda a k ‚àà {list(k_range)}\\n\")\nelse:\n    k_range = range(2, 11)  # B√∫squeda normal\n    print(f\"‚úì Dataset adecuado ({n_samples} muestras)\")\n    print(f\"   B√∫squeda normal: k ‚àà {list(k_range)}\\n\")\n\n# Ejecutar b√∫squeda\nk_results = analyzer.find_optimal_k(k_range=k_range)\n\n# Validar resultados\nif k_results.empty:\n    print(\"‚ùå Error: No se pudieron calcular m√©tricas de clustering\")\n    raise ValueError(\"find_optimal_k retorn√≥ tabla vac√≠a\")\n\nprint(\"‚úì B√∫squeda completada\")\ndisplay(k_results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar m√©tricas de k √≥ptimo\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0].plot(k_results['k'], k_results['silhouette'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)\n",
    "axes[0].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0].set_title('Silhouette Score vs k', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(k_results['k'])\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "axes[1].plot(k_results['k'], k_results['davies_bouldin'], 'rs-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)\n",
    "axes[1].set_ylabel('Davies-Bouldin Index', fontsize=11)\n",
    "axes[1].set_title('Davies-Bouldin Index vs k (menor es mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(k_results['k'])\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "axes[2].plot(k_results['k'], k_results['calinski_harabasz'], 'gs-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)\n",
    "axes[2].set_ylabel('Calinski-Harabasz Index', fontsize=11)\n",
    "axes[2].set_title('Calinski-Harabasz Index vs k', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_xticks(k_results['k'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/04_elbow_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/04_elbow_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar k √≥ptimo (basado en Silhouette Score)\n",
    "optimal_k = k_results.loc[k_results['silhouette'].idxmax(), 'k'].astype(int)\n",
    "print(f\"‚úì k √≥ptimo seleccionado: {optimal_k}\")\n",
    "print(f\"  Silhouette Score: {k_results.loc[k_results['k'] == optimal_k, 'silhouette'].values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means\n",
    "print(f\"\\n{'='*50}\")\n",
    "kmeans_labels, kmeans_metrics = analyzer.kmeans_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar Agglomerative Clustering\n",
    "print(f\"\\n{'='*50}\")\n",
    "agg_labels, agg_metrics = analyzer.agglomerative_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar DBSCAN\n",
    "print(f\"\\n{'='*50}\")\n",
    "dbscan_labels, dbscan_metrics = analyzer.dbscan_clustering(eps=0.8, min_samples=4)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de m√©tricas de clustering\n",
    "print(\"\\nüìä RESUMEN DE M√âTRICAS DE CLUSTERING\\n\")\n",
    "clustering_summary = analyzer.get_summary()\n",
    "display(clustering_summary)\n",
    "\n",
    "# Guardar resumen\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "print(\"\\n‚úì Resumen guardado: data/processed/clustering_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar clusters en t-SNE\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_labels,\n",
    "                           cmap='viridis', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[0].set_title(f'K-Means (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Agglomerative\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=agg_labels,\n",
    "                           cmap='plasma', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[1].set_title(f'Agglomerative Clustering (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# DBSCAN\n",
    "scatter3 = axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_labels,\n",
    "                           cmap='cool', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[2].set_title(f'DBSCAN', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('t-SNE 1')\n",
    "axes[2].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter3, ax=axes[2], label='Cluster')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/05_clustering_results_tsne.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/05_clustering_results_tsne.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n con ratings reales\n",
    "print(\"\\nüìä COMPARACI√ìN CON RATINGS REALES\\n\")\n",
    "\n",
    "# Codificar ratings a n√∫meros\n",
    "le = LabelEncoder()\n",
    "ratings_encoded = le.fit_transform(df['rating'])\n",
    "\n",
    "comparison = analyzer.compare_with_ratings(ratings_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n - K-Means vs Ratings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "algorithms = ['K-Means', 'Agglomerative', 'DBSCAN']\n",
    "labels_list = [kmeans_labels, agg_labels, dbscan_labels]\n",
    "\n",
    "for idx, (algo_name, labels) in enumerate(zip(algorithms, labels_list)):\n",
    "    cm = confusion_matrix(ratings_encoded, labels)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f'{algo_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Rating Real')\n",
    "    axes[idx].set_xlabel('Cluster Predicho')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/06_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/06_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ PARTE 4: SEMI-SUPERVISED LEARNING\n",
    "\n",
    "Comparamos diferentes enfoques variando el ratio de datos etiquetados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar learner semi-supervisado\n",
    "print(\"ü§ñ Inicializando Semi-Supervised Learner...\\n\")\n",
    "\n",
    "semi_learner = SemiSupervisedLearner(df[numeric_cols + ['rating']], \n",
    "                                      target_column='rating',\n",
    "                                      random_state=RANDOM_STATE)\n",
    "X_semi, y_semi = semi_learner.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline supervisado\n",
    "print(\"‚ñ∂ Entrenando BASELINE SUPERVISADO\\n\")\n",
    "baseline = semi_learner.supervised_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n variando ratios\n",
    "print(\"‚ñ∂ Evaluando Semi-Supervised Learning con diferentes ratios\\n\")\n",
    "\n",
    "ratios = [0.1, 0.2, 0.3, 0.5, 0.7]\n",
    "results_df = semi_learner.compare_ratios(ratios=ratios)\n",
    "\n",
    "print(\"\\n‚úì Evaluaci√≥n completada\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados semi-supervised\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"‚úì Resultados guardados: data/processed/semi_supervised_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n de m√©todos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "colors = {'Supervised Baseline': 'red', 'Label Propagation': 'blue', 'Self-Training': 'green'}\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Graficar baseline\n",
    "    baseline_value = results_df[results_df['method'] == 'Supervised Baseline'][metric].values[0]\n",
    "    ax.axhline(y=baseline_value, color='red', linestyle='--', linewidth=2, label='Supervised Baseline')\n",
    "    \n",
    "    # Graficar Label Propagation\n",
    "    lp_data = results_df[results_df['method'] == 'Label Propagation']\n",
    "    ax.plot(lp_data['labeled_ratio'] * 100, lp_data[metric], 'bo-', linewidth=2, \n",
    "            markersize=8, label='Label Propagation')\n",
    "    \n",
    "    # Graficar Self-Training\n",
    "    st_data = results_df[results_df['method'] == 'Self-Training']\n",
    "    ax.plot(st_data['labeled_ratio'] * 100, st_data[metric], 'gs-', linewidth=2, \n",
    "            markersize=8, label='Self-Training')\n",
    "    \n",
    "    ax.set_xlabel('Porcentaje de Datos Etiquetados (%)', fontsize=11)\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Ratio de Labels', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/07_semi_supervised_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ RESULTADOS Y CONCLUSIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úì N√∫mero √≥ptimo de clusters: {optimal_k}\")\n",
    "print(f\"\\nüìä M√©tricas por algoritmo:\")\n",
    "print(clustering_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI√ìN:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. SILHOUETTE SCORE (rango: -1 a 1)\n",
    "   - Mide la similitud de un objeto con su cluster vs otros clusters\n",
    "   - Valores m√°s altos indican mejor separaci√≥n\n",
    "   - Interpretaci√≥n: > 0.5 (bueno), > 0.7 (excelente)\n",
    "\n",
    "2. DAVIES-BOULDIN INDEX (menor es mejor)\n",
    "   - Raz√≥n promedio de similitud intra-cluster vs inter-cluster\n",
    "   - Valores m√°s bajos indican clusters mejor definidos\n",
    "   - Interpretaci√≥n: < 1.0 (bueno), < 0.5 (excelente)\n",
    "\n",
    "3. COMPARACI√ìN CON RATINGS REALES\n",
    "   - Adjusted Rand Index mide acuerdo entre clustering y ratings\n",
    "   - Rango: -1 a 1 (1 = acuerdo perfecto, 0 = acuerdo aleatorio)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - SEMI-SUPERVISED LEARNING\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI√ìN:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. ACCURACY\n",
    "   - Proporci√≥n de predicciones correctas\n",
    "   - M√©trica general de rendimiento\n",
    "\n",
    "2. PRECISION\n",
    "   - Proporci√≥n de predicciones positivas que fueron correctas\n",
    "   - Importante cuando el costo de falsos positivos es alto\n",
    "\n",
    "3. RECALL\n",
    "   - Proporci√≥n de casos positivos que fueron identificados\n",
    "   - Importante cuando el costo de falsos negativos es alto\n",
    "\n",
    "4. F1-SCORE\n",
    "   - Media arm√≥nica entre Precision y Recall\n",
    "   - M√©trica equilibrada para clasificaci√≥n desbalanceada\n",
    "\n",
    "5. RATIO DE LABELS\n",
    "   - Proporci√≥n de datos etiquetados usados en entrenamiento\n",
    "   - Medir el impacto de tener menos datos etiquetados\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de clusters vs ratings\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AN√ÅLISIS DETALLADO: CLUSTERS K-MEANS vs RATINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "df_clustered = df.copy()\n",
    "df_clustered['cluster_kmeans'] = kmeans_labels\n",
    "df_clustered['cluster_agg'] = agg_labels\n",
    "df_clustered['cluster_dbscan'] = dbscan_labels\n",
    "\n",
    "print(\"\\nüìä Distribuci√≥n de ratings por cluster K-Means:\")\n",
    "crosstab = pd.crosstab(df_clustered['rating'], df_clustered['cluster_kmeans'], margins=True)\n",
    "print(crosstab)\n",
    "\n",
    "print(\"\\nüí° Observaciones:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['cluster_kmeans'] == cluster]\n",
    "    rating_dist = cluster_data['rating'].value_counts()\n",
    "    print(f\"  Cluster {cluster}: {len(cluster_data)} cooperativas\")\n",
    "    print(f\"    Distribuci√≥n de ratings: {dict(rating_dist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusiones finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSIONES Y RECOMENDACIONES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "üéØ HALLAZGOS PRINCIPALES:\n",
    "\n",
    "1. CLUSTERING NO SUPERVISADO:\n",
    "   ‚Ä¢ Se identificaron patrones naturales en los datos financieros\n",
    "   ‚Ä¢ El n√∫mero √≥ptimo de clusters fue determinado mediante Silhouette Score\n",
    "   ‚Ä¢ K-Means proporciona una buena separaci√≥n de cooperativas\n",
    "   ‚Ä¢ Los clusters muestran cierta coherencia con los ratings reales\n",
    "\n",
    "2. COMPARACI√ìN CON RATINGS REALES:\n",
    "   ‚Ä¢ Existe una relaci√≥n parcial entre clusters y ratings\n",
    "   ‚Ä¢ Algunos ratings se distribuyen en m√∫ltiples clusters\n",
    "   ‚Ä¢ Sugiere que los indicadores financieros capturan matices no reflejados en ratings simples\n",
    "\n",
    "3. SEMI-SUPERVISED LEARNING:\n",
    "   ‚Ä¢ Label Propagation muestra mejor rendimiento con menos datos etiquetados\n",
    "   ‚Ä¢ Self-Training es m√°s inestable en ratios bajos\n",
    "   ‚Ä¢ Ambos m√©todos se acercan al baseline supervisado con ~50% de datos etiquetados\n",
    "\n",
    "üìå RECOMENDACIONES:\n",
    "\n",
    "   1. Para clasificaci√≥n de nuevas cooperativas:\n",
    "      ‚Üí Usar modelo supervisado con todos los datos disponibles\n",
    "      ‚Üí Si hay nuevas cooperativas sin etiquetar, aplicar Label Propagation\n",
    "\n",
    "   2. Para segmentaci√≥n de cooperativas:\n",
    "      ‚Üí K-Means proporciona grupos interpretables\n",
    "      ‚Üí Validar grupos con expertos en finanzas\n",
    "\n",
    "   3. Mejoras futuras:\n",
    "      ‚Üí Incluir m√°s indicadores financieros\n",
    "      ‚Üí Validaci√≥n cruzada temporal (datos hist√≥ricos)\n",
    "      ‚Üí An√°lisis de estabilidad de clusters\n",
    "      ‚Üí Investigar por qu√© algunos ratings se distribuyen en m√∫ltiples clusters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS FINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar datos clustered\n",
    "df_clustered.to_csv('data/processed/cooperativas_clustered.csv', index=False)\n",
    "print(\"‚úì Datos clustered guardados\")\n",
    "\n",
    "# Guardar m√©tricas\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"‚úì M√©tricas guardadas\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis completado exitosamente\")\n",
    "print(\"\\nüìÅ Archivos generados:\")\n",
    "print(\"  ‚Ä¢ data/processed/cooperativas_data.csv\")\n",
    "print(\"  ‚Ä¢ data/processed/cooperativas_clustered.csv\")\n",
    "print(\"  ‚Ä¢ data/processed/clustering_metrics.csv\")\n",
    "print(\"  ‚Ä¢ data/processed/semi_supervised_results.csv\")\n",
    "print(\"  ‚Ä¢ figures/01_distribucion_por_rating.png\")\n",
    "print(\"  ‚Ä¢ figures/02_matriz_correlacion.png\")\n",
    "print(\"  ‚Ä¢ figures/03_tsne_visualization.png\")\n",
    "print(\"  ‚Ä¢ figures/04_elbow_analysis.png\")\n",
    "print(\"  ‚Ä¢ figures/05_clustering_results_tsne.png\")\n",
    "print(\"  ‚Ä¢ figures/06_confusion_matrices.png\")\n",
    "print(\"  ‚Ä¢ figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Referencias y Metodolog√≠a\n",
    "\n",
    "### Fuentes Te√≥ricas\n",
    "\n",
    "1. **Clustering No Supervisado:**\n",
    "   - Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory\n",
    "   - Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation of cluster analysis\n",
    "   - Davies, D. L., & Bouldin, D. W. (1979). A cluster separation measure\n",
    "\n",
    "2. **Semi-Supervised Learning:**\n",
    "   - Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Sch√∂lkopf, B. (2004). Learning with local and global consistency\n",
    "   - Rosenberg, D., Hebert, M., & Schneiderman, H. (2005). Semi-supervised self-training of object detection models\n",
    "\n",
    "3. **Visualizaci√≥n:**\n",
    "   - van der Maaten, L., & Hinton, G. (2008). Visualizing Data using t-SNE\n",
    "\n",
    "### Indicadores Financieros\n",
    "\n",
    "Referencia: Superintendencia de Econom√≠a Popular y Solidaria (SEPS)\n",
    "- https://www.seps.gob.ec\n",
    "- ASIS: Asociaci√≥n de Supervisores de Instituciones de Seguros\n",
    "- https://www.asis.fin.ec\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook generado:** Noviembre 2025\n",
    "\n",
    "**Pr√≥ximos pasos:**\n",
    "1. Obtener datos reales de cooperativas (archivos PDF)\n",
    "2. Validar resultados con expertos en finanzas\n",
    "3. Realizar an√°lisis temporal de estabilidad de clusters\n",
    "4. Investigar casos discrepantes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}