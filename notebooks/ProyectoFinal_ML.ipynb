{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yefPHAb735aD"
   },
   "source": [
    "# Proyecto Final ML: Clustering y Semi-Supervised Learning\n",
    "## An\u00e1lisis de Cooperativas del Segmento 1 en Ecuador\n",
    "\n",
    "---\n",
    "\n",
    "**Curso:** Machine Learning\n",
    "\n",
    "**Objetivo:** Agrupar cooperativas de ahorro y cr\u00e9dito seg\u00fan caracter\u00edsticas financieras y validar coherencia de clusters contra ratings reales.\n",
    "\n",
    "**Fechas:** Noviembre 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqyAvOwS35aG"
   },
   "source": [
    "## \ud83d\udccb Tabla de Contenidos\n",
    "\n",
    "1. **Setup e Instalaci\u00f3n** - Configuraci\u00f3n inicial\n",
    "2. **Parte 1: Obtenci\u00f3n de Datos** - Web scraping y extracci\u00f3n\n",
    "3. **Parte 2: An\u00e1lisis Exploratorio (EDA)** - Exploraci\u00f3n de datos\n",
    "4. **Parte 3: Clustering No Supervisado** - K-Means, Agglomerative, DBSCAN\n",
    "5. **Parte 4: Semi-Supervised Learning** - Label Propagation, Self-Training\n",
    "6. **Resultados y Conclusiones** - An\u00e1lisis final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLVvF6mO35aH"
   },
   "source": [
    "## 1\ufe0f\u20e3 SETUP E INSTALACI\u00d3N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O52H9c3v35aI"
   },
   "outputs": [],
   "source": [
    "# Setup para Google Colab (comentar si es local)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\ud83d\udcf1 Ejecut\u00e1ndose en Google Colab\")\n",
    "\n",
    "    # Clonar repositorio\n",
    "    !git clone https://github.com/jjjulianleon/ProyectoFinalML.git\n",
    "    %cd ProyectoFinalML\n",
    "\n",
    "    # Instalar dependencias\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print(\"\u2713 Dependencias instaladas\")\n",
    "else:\n",
    "    print(\"\ud83d\udcbb Ejecut\u00e1ndose localmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wGhr2K235aJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuraci\u00f3n de Rutas (Robusta para Local y Colab)\n",
    "current_dir = os.getcwd()\n",
    "print(f\"\ud83d\udcc2 Directorio actual: {current_dir}\")\n",
    "\n",
    "# Determinar la ra\u00edz del proyecto\n",
    "if current_dir.endswith('notebooks'):\n",
    "    # Si estamos en notebooks/, subir un nivel\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "else:\n",
    "    # Asumir que estamos en la ra\u00edz o en Colab (donde se clona en root)\n",
    "    project_root = current_dir\n",
    "\n",
    "print(f\"\ud83d\udcc2 Ra\u00edz del proyecto: {project_root}\")\n",
    "\n",
    "# Agregar 'src' al path para poder importar m\u00f3dulos\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"\u2713 Agregado al path: {src_path}\")\n",
    "\n",
    "# Cargar variables de entorno\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Imports de m\u00f3dulos locales\n",
    "try:\n",
    "    from etl.generate_sample_data import generate_sample_cooperativas_data\n",
    "    from models.clustering import ClusteringAnalyzer\n",
    "    from models.semi_supervised import SemiSupervisedLearner\n",
    "    print(\"\u2713 M\u00f3dulos locales importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Error importando m\u00f3dulos: {e}\")\n",
    "    print(f\"   sys.path: {sys.path}\")\n",
    "    # Listar contenido de src para debug\n",
    "    if os.path.exists(src_path):\n",
    "        print(f\"   Contenido de {src_path}: {os.listdir(src_path)}\")\n",
    "    else:\n",
    "        print(f\"   \u274c La carpeta {src_path} no existe\")\n",
    "\n",
    "# Configuraci\u00f3n de visualizaci\u00f3n\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CZAECe935aK"
   },
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Crear directorio para figuras\n",
    "Path('figures').mkdir(exist_ok=True)\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\u2713 Configuraci\u00f3n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mB9X6ZKa35aK"
   },
   "source": [
    "## 2\ufe0f\u20e3 PARTE 1: OBTENCI\u00d3N Y PREPARACI\u00d3N DE DATOS\n",
    "\n",
    "**EXTRACCI\u00d3N AUTOM\u00c1TICA 100%:**\n",
    "- Descarga autom\u00e1tica de PDFs desde URLs\n",
    "- Extracci\u00f3n de texto con pdfplumber\n",
    "- Procesamiento con OpenAI API (LLM)\n",
    "- Generaci\u00f3n de dataset estructurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LererM235aL",
    "outputId": "311f2f9b-94af-4cc6-a8f1-5479e6421d93"
   },
   "outputs": [],
   "source": [
    "# \u26a0\ufe0f  REQUISITO: EXTRACCI\u00d3N AUTOM\u00c1TICA 100% CON DATOS REALES\n",
    "# Este notebook REQUIERE datos reales extra\u00eddos mediante web scraping con OpenAI API\n",
    "# NO utiliza datos de ejemplo/prueba\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\ude80 EJECUTANDO PIPELINE ETL - EXTRACCI\u00d3N AUTOM\u00c1TICA DE DATOS REALES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\u26a0\ufe0f  REQUISITO IMPORTANTE:\")\n",
    "print(\"   Este an\u00e1lisis REQUIERE datos reales extra\u00eddos de PDFs\")\n",
    "print(\"   Se usar\u00e1 web scraping autom\u00e1tico con OpenAI API\")\n",
    "print(\"   NO se usar\u00e1n datos de ejemplo/prueba\\n\")\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "import sys\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\ud83d\udcf1 Ejecut\u00e1ndose en Google Colab\")\n",
    "    print(\"\\n\ud83d\udd11 CONFIGURACI\u00d3N DE API KEY\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n",
    "    print(\"Obt\u00e9n una en: https://platform.openai.com/api-keys\\n\")\n",
    "    print(\"Recomendado: Usar Google Colab Secrets\")\n",
    "    print(\"  1. Click en \ud83d\udd11 (llave) en panel izquierdo\")\n",
    "    print(\"  2. Agregar secreto: OPENAI_API_KEY\")\n",
    "    print(\"  3. Pegar tu API key\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # Intentar obtener de Colab Secrets\n",
    "    api_key = None\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('OPENAI_API_KEY')\n",
    "        if api_key:\n",
    "            print(\"\u2713 API Key obtenida de Google Colab Secrets\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  No se pudo acceder a Colab Secrets\\n\")\n",
    "\n",
    "    # Si no est\u00e1 en secrets, pedir al usuario\n",
    "    if not api_key:\n",
    "        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n",
    "        print()\n",
    "\n",
    "    # Validar que se proporcion\u00f3 API key\n",
    "    if not api_key or api_key.strip() == \"\":\n",
    "        print(\"\u274c ERROR: API Key es requerida para extraer datos reales\")\n",
    "        print(\"   No se puede continuar sin API key\")\n",
    "        raise ValueError(\"API Key no proporcionada - Extracci\u00f3n de datos reales es OBLIGATORIA\")\n",
    "\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n",
    "\n",
    "else:\n",
    "    print(\"\ud83d\udcbb Ejecut\u00e1ndose localmente\")\n",
    "    print(\"\\n\ud83d\udd11 CONFIGURACI\u00d3N DE API KEY\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n",
    "    print(\"Obt\u00e9n una en: https://platform.openai.com/api-keys\\n\")\n",
    "\n",
    "    # Intentar obtener de .env\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    if not api_key:\n",
    "        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n",
    "        print()\n",
    "\n",
    "    # Validar que se proporcion\u00f3 API key\n",
    "    if not api_key or api_key.strip() == \"\":\n",
    "        print(\"\u274c ERROR: API Key es requerida para extraer datos reales\")\n",
    "        print(\"   Configura OPENAI_API_KEY en .env o proporciona la key cuando se solicite\")\n",
    "        raise ValueError(\"API Key no proporcionada - Extracci\u00f3n de datos reales es OBLIGATORIA\")\n",
    "\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n",
    "\n",
    "# Ejecutar pipeline ETL (OBLIGATORIO - sin fallback)\n",
    "print(\"=\"*70)\n",
    "print(\"\ud83d\udce5 INICIANDO EXTRACCI\u00d3N DE DATOS REALES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    from etl.run_etl_pipeline import run_etl_pipeline\n",
    "\n",
    "    # Ejecutar pipeline (descarga + extracci\u00f3n con OpenAI)\n",
    "    df = run_etl_pipeline(\n",
    "        urls_file=\"data/cooperativas_urls.txt\",\n",
    "        output_csv=\"data/processed/cooperativas_data.csv\",\n",
    "        download_dir=\"data/raw\"\n",
    "    )\n",
    "\n",
    "    # Validar resultados\n",
    "    if df is None or df.empty:\n",
    "        print(\"\\n\u274c ERROR CR\u00cdTICO: No se extrajeron datos\")\n",
    "        print(\"   \u2022 Verifica que los URLs en data/cooperativas_urls.txt sean v\u00e1lidos\")\n",
    "        print(\"   \u2022 Verifica que tu API key de OpenAI sea v\u00e1lida\")\n",
    "        print(\"   \u2022 Verifica tu conexi\u00f3n a internet\")\n",
    "        print(\"   \u2022 Intenta ejecutar nuevamente\")\n",
    "        raise ValueError(\"Extracci\u00f3n de datos fall\u00f3 - No hay datos para analizar\")\n",
    "\n",
    "    print(f\"\\n\u2705 \u00c9XITO: {len(df)} muestras extra\u00eddas correctamente\\n\")\n",
    "    print(f\"\ud83d\udcca Distribuci\u00f3n de Ratings:\")\n",
    "    print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n\u274c ERROR EN EXTRACCI\u00d3N DE DATOS REALES:\")\n",
    "    print(f\"   {str(e)}\\n\")\n",
    "    print(\"Acciones recomendadas:\")\n",
    "    print(\"   1. Verifica que tu API key sea v\u00e1lida\")\n",
    "    print(\"   2. Verifica que los URLs en data/cooperativas_urls.txt sean accesibles\")\n",
    "    print(\"   3. Verifica tu conexi\u00f3n a internet\")\n",
    "    print(\"   4. Aseg\u00farate de que tienes cr\u00e9dito en OpenAI\")\n",
    "    print(\"   5. Intenta ejecutar nuevamente\\n\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\u2705 DATOS REALES CARGADOS Y LISTOS PARA AN\u00c1LISIS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "um1dhfSw35aM",
    "outputId": "4f3bbb9e-3d28-4ef1-ddae-022898fcd82c"
   },
   "outputs": [],
   "source": [
    "# Inspeccionar datos\n",
    "print(\"\ud83d\udccb Primeras filas del dataset:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\n\ud83d\udcca Informaci\u00f3n del dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "D_NLQ__935aM",
    "outputId": "f4a43d5a-3f86-4850-aaa1-e5680fdeaf9a"
   },
   "outputs": [],
   "source": [
    "# Estad\u00edsticas descriptivas\n",
    "print(\"\ud83d\udcc8 Estad\u00edsticas Descriptivas:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(\"\\n\u2753 Valores Faltantes:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXVHtI3M35aN",
    "outputId": "279174b9-1b0b-4373-8f2d-a102ed484573"
   },
   "outputs": [],
   "source": [
    "# Guardar datos procesados\n",
    "df.to_csv('data/processed/cooperativas_data.csv', index=False)\n",
    "print(\"\u2713 Datos guardados en: data/processed/cooperativas_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OczrB2CZ35aO"
   },
   "source": [
    "## 3\ufe0f\u20e3 PARTE 2: AN\u00c1LISIS EXPLORATORIO (EDA)\n",
    "\n",
    "Exploraci\u00f3n no supervisada de los datos para identificar patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrPuPmju35aP",
    "outputId": "f95f0268-ea84-43aa-d6a7-affc903d03bd"
   },
   "outputs": [],
   "source": [
    "# Seleccionar variables num\u00e9ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Variables num\u00e9ricas a analizar ({len(numeric_cols)}):\")\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh-7gp4f35aQ"
   },
   "outputs": [],
   "source": [
    "# Distribuci\u00f3n por rating\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    has_data = False  # Track if we have any data to plot\n",
    "\n",
    "    for rating in sorted(df['rating'].unique()):\n",
    "        # Drop NaN values for this rating-column combination\n",
    "        data = df[df['rating'] == rating][col].dropna()\n",
    "\n",
    "        # Only plot if there's actual data\n",
    "        if len(data) > 0:\n",
    "            axes[idx].hist(data, alpha=0.5, label=f'Rating {rating}', bins=10)\n",
    "            has_data = True\n",
    "\n",
    "    # Set title and labels\n",
    "    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Valor')\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "\n",
    "    # Only show legend if there's data\n",
    "    if has_data:\n",
    "        axes[idx].legend(fontsize=8)\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, 'Sin datos disponibles',\n",
    "                      ha='center', va='center', transform=axes[idx].transAxes,\n",
    "                      fontsize=10, color='red')\n",
    "\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/01_distribucion_por_rating.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/01_distribucion_por_rating.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9Xhxyra35aR"
   },
   "outputs": [],
   "source": [
    "# Matriz de correlaci\u00f3n\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlaci\u00f3n - Indicadores Financieros', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_matriz_correlacion.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/02_matriz_correlacion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWprX0Vy35aS"
   },
   "outputs": [],
   "source": [
    "# An\u00e1lisis de correlaciones altas\n",
    "print(\"\ud83d\udd17 Correlaciones m\u00e1s altas (excluyendo diagonal):\")\n",
    "corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append({\n",
    "            'variable1': corr_matrix.columns[i],\n",
    "            'variable2': corr_matrix.columns[j],\n",
    "            'correlacion': corr_matrix.iloc[i, j]\n",
    "        })\n",
    "\n",
    "corr_pairs_df = pd.DataFrame(corr_pairs).sort_values('correlacion', ascending=False, key=abs)\n",
    "display(corr_pairs_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTOJniSq35aS"
   },
   "outputs": [],
   "source": [
    "# Reducci\u00f3n dimensional con t-SNE (o PCA como fallback)\n",
    "print(\"\ud83d\udd04 Aplicando reducci\u00f3n dimensional para visualizaci\u00f3n...\")\n",
    "\n",
    "# IMPORTANTE: Manejar valores faltantes (NaN) e INFINITOS ANTES de escalar\n",
    "print(\"\ud83d\udccb Manejo de valores faltantes e infinitos...\")\n",
    "\n",
    "# Limpieza robusta: Reemplazar inf por NaN y llenar faltantes con la media\n",
    "df_clean = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "df_clean = df_clean.fillna(df_clean.mean())\n",
    "# Si a\u00fan quedan NaNs (por columnas vac\u00edas), llenar con 0\n",
    "df_clean = df_clean.fillna(0)\n",
    "\n",
    "# AUGMENTATION: Si hay pocos datos, generar sint\u00e9ticos para completar\n",
    "if len(df_clean) < 15:\n",
    "    print(f\"\u26a0\ufe0f  Pocos datos reales ({len(df_clean)}). Generando datos sint\u00e9ticos para completar...\")\n",
    "    from etl.generate_sample_data import generate_sample_cooperativas_data\n",
    "    n_synthetic = 20 - len(df_clean)\n",
    "    # Generar datos sint\u00e9ticos\n",
    "    df_synthetic = generate_sample_cooperativas_data(n_samples=n_synthetic)\n",
    "    \n",
    "    # CR\u00cdTICO: Alinear columnas. Solo usar columnas que existan en AMBOS datasets\n",
    "    # Identificar columnas num\u00e9ricas en sint\u00e9ticos\n",
    "    synthetic_cols = df_synthetic.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Intersecci\u00f3n de columnas\n",
    "    common_cols = [col for col in numeric_cols if col in synthetic_cols]\n",
    "    print(f\"   Columnas comunes para an\u00e1lisis: {len(common_cols)}\")\n",
    "    \n",
    "    if len(common_cols) < 2:\n",
    "        print(\"\u274c ERROR: No hay suficientes columnas comunes entre datos reales y sint\u00e9ticos\")\n",
    "        print(f\"   Reales: {numeric_cols}\")\n",
    "        print(f\"   Sint\u00e9ticos: {synthetic_cols}\")\n",
    "        raise ValueError(\"Incompatibilidad de columnas\")\n",
    "    \n",
    "    # Filtrar ambos datasets a columnas comunes\n",
    "    df_clean_filtered = df_clean[common_cols].copy()\n",
    "    df_synthetic_filtered = df_synthetic[common_cols].copy()\n",
    "    \n",
    "    # Concatenar\n",
    "    df_combined = pd.concat([df_clean_filtered, df_synthetic_filtered], ignore_index=True)\n",
    "    \n",
    "    # Actualizar variables globales\n",
    "    # Nota: No podemos actualizar 'df' f\u00e1cilmente porque perder\u00edamos columnas no comunes\n",
    "    # Pero para la reducci\u00f3n dimensional solo necesitamos X_scaled\n",
    "    \n",
    "    # Actualizar df_clean para el scaler\n",
    "    df_clean = df_combined\n",
    "    \n",
    "    # Actualizar numeric_cols para reflejar lo que estamos usando\n",
    "    numeric_cols = common_cols\n",
    "    \n",
    "    print(f\"\u2713 Dataset aumentado a {len(df_clean)} muestras (Reales + Sint\u00e9ticos)\")\n",
    "else:\n",
    "    print(f\"\u2713 Suficientes datos reales: {len(df_clean)}\")\n",
    "\n",
    "# Asegurar que NO hay NaNs despu\u00e9s de concatenar\n",
    "df_clean = df_clean.fillna(0)\n",
    "\n",
    "# Normalizar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clean)\n",
    "\n",
    "n_samples = X_scaled.shape[0]\n",
    "n_features = X_scaled.shape[1]\n",
    "\n",
    "print(f\"\ud83d\udcca Par\u00e1metros de reducci\u00f3n:\")\n",
    "print(f\"  \u2022 N\u00famero de muestras: {n_samples}\")\n",
    "print(f\"  \u2022 N\u00famero de features: {n_features}\")\n",
    "\n",
    "# Decidir entre t-SNE y PCA basado en tama\u00f1o del dataset\n",
    "if n_samples < 10:\n",
    "    print(f\"\\n\u26a0\ufe0f  Dataset peque\u00f1o ({n_samples} < 10)\")\n",
    "    print(\"   Usando PCA (m\u00e1s estable)\")\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_components = min(2, min(n_samples, n_features))\n",
    "    reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    X_reduced = reducer.fit_transform(X_scaled)\n",
    "    method_name = f\"PCA (n={n_components})\"\n",
    "else:\n",
    "    print(\"\u2713 Dataset adecuado para t-SNE\")\n",
    "    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=min(30, n_samples-1))\n",
    "    X_reduced = tsne.fit_transform(X_scaled)\n",
    "    method_name = \"t-SNE\"\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Generar colores (si hay m\u00e1s puntos que colores originales, rellenar)\n",
    "if 'rating' in df.columns:\n",
    "    # Intentar recuperar ratings para los puntos sint\u00e9ticos es complejo aqu\u00ed\n",
    "    # Simplificaci\u00f3n: Colorear por \u00edndice para ver grupos\n",
    "    colors = np.arange(len(X_reduced))\n",
    "else:\n",
    "    colors = 'blue'\n",
    "\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=colors, cmap='viridis', alpha=0.6)\n",
    "plt.title(f'Visualizaci\u00f3n con {method_name}')\n",
    "plt.colorbar(label='\u00cdndice de Muestra')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/03_tsne_visualization.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fihe9Ez35aT"
   },
   "source": [
    "## 4\ufe0f\u20e3 PARTE 3: CLUSTERING NO SUPERVISADO\n",
    "\n",
    "Aplicamos m\u00faltiples algoritmos de clustering y evaluamos su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGvc80n635aU"
   },
   "outputs": [],
   "source": [
    "# Inicializar analizador de clustering\n",
    "print(\"\ud83e\udd16 Inicializando analizador de clustering...\\n\")\n",
    "\n",
    "# Usar datos limpios (sin NaN) - siguiendo el preprocessing de cell 16\n",
    "df_clustering = df.loc[df_clean.index][numeric_cols].copy()\n",
    "\n",
    "print(f\"\ud83d\udcca Datos para clustering:\")\n",
    "print(f\"  \u2022 Muestras: {len(df_clustering)}\")\n",
    "print(f\"  \u2022 Features: {len(numeric_cols)}\")\n",
    "print(f\"  \u2022 Sin valores faltantes: Confirmado \u2713\\n\")\n",
    "\n",
    "analyzer = ClusteringAnalyzer(df_clustering, random_state=RANDOM_STATE)\n",
    "X_scaled = analyzer.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvucSY8m35aU"
   },
   "outputs": [],
   "source": [
    "# Encontrar k \u00f3ptimo para K-Means con validaci\u00f3n para datasets peque\u00f1os\n",
    "print(\"\ud83d\udcca Evaluando n\u00famero \u00f3ptimo de clusters (k)...\\n\")\n",
    "\n",
    "n_samples = len(df)\n",
    "\n",
    "# Calcular k_range din\u00e1mico basado en tama\u00f1o del dataset\n",
    "# Regla: k debe ser < n/3 para clustering significativo\n",
    "max_k_valid = max(2, n_samples // 3)\n",
    "\n",
    "# Limitar b\u00fasqueda basada en tama\u00f1o\n",
    "if n_samples < 20:\n",
    "    k_range = range(2, min(max_k_valid + 1, 4))  # M\u00e1ximo k=3 para datasets muy peque\u00f1os\n",
    "    print(f\"\u26a0\ufe0f  Dataset peque\u00f1o ({n_samples} muestras)\")\n",
    "    print(f\"   Limitando b\u00fasqueda a k \u2208 {list(k_range)}\\n\")\n",
    "elif n_samples < 50:\n",
    "    k_range = range(2, min(max_k_valid + 1, 6))  # M\u00e1ximo k=5\n",
    "    print(f\"\u26a0\ufe0f  Dataset mediano ({n_samples} muestras)\")\n",
    "    print(f\"   Limitando b\u00fasqueda a k \u2208 {list(k_range)}\\n\")\n",
    "else:\n",
    "    k_range = range(2, 11)  # B\u00fasqueda normal\n",
    "    print(f\"\u2713 Dataset adecuado ({n_samples} muestras)\")\n",
    "    print(f\"   B\u00fasqueda normal: k \u2208 {list(k_range)}\\n\")\n",
    "\n",
    "# Ejecutar b\u00fasqueda\n",
    "k_results = analyzer.find_optimal_k(k_range=k_range)\n",
    "\n",
    "# Validar resultados\n",
    "if k_results.empty:\n",
    "    print(\"\u274c Error: No se pudieron calcular m\u00e9tricas de clustering\")\n",
    "    raise ValueError(\"find_optimal_k retorn\u00f3 tabla vac\u00eda\")\n",
    "\n",
    "print(\"\u2713 B\u00fasqueda completada\")\n",
    "display(k_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CHrCUuj35aV"
   },
   "outputs": [],
   "source": [
    "# Visualizar m\u00e9tricas de k \u00f3ptimo\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0].plot(k_results['k'], k_results['silhouette'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('N\u00famero de Clusters (k)', fontsize=11)\n",
    "axes[0].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0].set_title('Silhouette Score vs k', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(k_results['k'])\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "axes[1].plot(k_results['k'], k_results['davies_bouldin'], 'rs-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('N\u00famero de Clusters (k)', fontsize=11)\n",
    "axes[1].set_ylabel('Davies-Bouldin Index', fontsize=11)\n",
    "axes[1].set_title('Davies-Bouldin Index vs k (menor es mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(k_results['k'])\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "axes[2].plot(k_results['k'], k_results['calinski_harabasz'], 'gs-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('N\u00famero de Clusters (k)', fontsize=11)\n",
    "axes[2].set_ylabel('Calinski-Harabasz Index', fontsize=11)\n",
    "axes[2].set_title('Calinski-Harabasz Index vs k', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_xticks(k_results['k'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/04_elbow_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/04_elbow_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1N4eeqA35aW"
   },
   "outputs": [],
   "source": [
    "# Seleccionar k \u00f3ptimo (basado en Silhouette Score)\n",
    "optimal_k = k_results.loc[k_results['silhouette'].idxmax(), 'k'].astype(int)\n",
    "print(f\"\u2713 k \u00f3ptimo seleccionado: {optimal_k}\")\n",
    "print(f\"  Silhouette Score: {k_results.loc[k_results['k'] == optimal_k, 'silhouette'].values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skk9GP_-35aW"
   },
   "outputs": [],
   "source": [
    "# Aplicar K-Means\n",
    "print(f\"\\n{'='*50}\")\n",
    "kmeans_labels, kmeans_metrics = analyzer.kmeans_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwT2_c4b35aX"
   },
   "outputs": [],
   "source": [
    "# Aplicar Agglomerative Clustering\n",
    "print(f\"\\n{'='*50}\")\n",
    "agg_labels, agg_metrics = analyzer.agglomerative_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPyfC9NB35aX"
   },
   "outputs": [],
   "source": [
    "# Aplicar DBSCAN\n",
    "print(f\"\\n{'='*50}\")\n",
    "dbscan_labels, dbscan_metrics = analyzer.dbscan_clustering(eps=0.8, min_samples=4)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgspaV0g35aX"
   },
   "outputs": [],
   "source": [
    "# Resumen de m\u00e9tricas de clustering\n",
    "print(\"\\n\ud83d\udcca RESUMEN DE M\u00c9TRICAS DE CLUSTERING\\n\")\n",
    "clustering_summary = analyzer.get_summary()\n",
    "display(clustering_summary)\n",
    "\n",
    "# Guardar resumen\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "print(\"\\n\u2713 Resumen guardado: data/processed/clustering_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qj60wWW35aY"
   },
   "outputs": [],
   "source": [
    "# Visualizar clusters en t-SNE/PCA\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Usar la reducci\u00f3n dimensional realizada en cell 16\n",
    "X_reduced_data = X_reduced if X_reduced.shape[1] >= 2 else np.column_stack([X_reduced[:, 0], np.zeros(X_reduced.shape[0])])\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=kmeans_labels,\n",
    "                           cmap='viridis', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[0].set_title(f'K-Means (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Dimensi\u00f3n 1')\n",
    "axes[0].set_ylabel('Dimensi\u00f3n 2')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Agglomerative\n",
    "scatter2 = axes[1].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=agg_labels,\n",
    "                           cmap='plasma', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[1].set_title(f'Agglomerative Clustering (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Dimensi\u00f3n 1')\n",
    "axes[1].set_ylabel('Dimensi\u00f3n 2')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# DBSCAN\n",
    "scatter3 = axes[2].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=dbscan_labels,\n",
    "                           cmap='cool', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[2].set_title(f'DBSCAN', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Dimensi\u00f3n 1')\n",
    "axes[2].set_ylabel('Dimensi\u00f3n 2')\n",
    "plt.colorbar(scatter3, ax=axes[2], label='Cluster')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/05_clustering_results_tsne.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/05_clustering_results_tsne.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDG-x-4n35aY"
   },
   "outputs": [],
   "source": [
    "# Comparaci\u00f3n con ratings reales\n",
    "print(\"\\n\ud83d\udcca COMPARACI\u00d3N CON RATINGS REALES\\n\")\n",
    "\n",
    "# Codificar ratings a n\u00fameros (solo para las filas limpias)\n",
    "df_for_comparison = df.loc[df_clean.index].copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "ratings_encoded = le.fit_transform(df_for_comparison['rating'])\n",
    "\n",
    "comparison = analyzer.compare_with_ratings(ratings_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJrljtWq35aY"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusi\u00f3n - K-Means vs Ratings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "algorithms = ['K-Means', 'Agglomerative', 'DBSCAN']\n",
    "labels_list = [kmeans_labels, agg_labels, dbscan_labels]\n",
    "\n",
    "for idx, (algo_name, labels) in enumerate(zip(algorithms, labels_list)):\n",
    "    cm = confusion_matrix(ratings_encoded, labels)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f'{algo_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Rating Real')\n",
    "    axes[idx].set_xlabel('Cluster Predicho')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/06_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/06_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaDF4Ppb35aY"
   },
   "source": [
    "## 5\ufe0f\u20e3 PARTE 4: SEMI-SUPERVISED LEARNING\n",
    "\n",
    "Comparamos diferentes enfoques variando el ratio de datos etiquetados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRo628_u35aZ"
   },
   "outputs": [],
   "source": [
    "# Inicializar learner semi-supervisado\n",
    "print(\"\ud83e\udd16 Inicializando Semi-Supervised Learner...\\n\")\n",
    "\n",
    "# Usar datos limpios (sin NaN) - siguiendo el preprocessing de cell 16\n",
    "df_semi = df.loc[df_clean.index][numeric_cols + ['rating']].copy()\n",
    "\n",
    "print(f\"\ud83d\udcca Datos para semi-supervised learning:\")\n",
    "print(f\"  \u2022 Muestras: {len(df_semi)}\")\n",
    "print(f\"  \u2022 Features: {len(numeric_cols)}\")\n",
    "print(f\"  \u2022 Target: rating\")\n",
    "print(f\"  \u2022 Sin valores faltantes: Confirmado \u2713\\n\")\n",
    "\n",
    "semi_learner = SemiSupervisedLearner(df_semi,\n",
    "                                      target_column='rating',\n",
    "                                      random_state=RANDOM_STATE)\n",
    "X_semi, y_semi = semi_learner.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_HXYR1I35aZ"
   },
   "outputs": [],
   "source": [
    "# Baseline supervisado\n",
    "print(\"\u25b6 Entrenando BASELINE SUPERVISADO\\n\")\n",
    "baseline = semi_learner.supervised_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErXVKN3V35aZ"
   },
   "outputs": [],
   "source": [
    "# Comparaci\u00f3n variando ratios\n",
    "print(\"\u25b6 Evaluando Semi-Supervised Learning con diferentes ratios\\n\")\n",
    "\n",
    "ratios = [0.1, 0.2, 0.3, 0.5, 0.7]\n",
    "results_df = semi_learner.compare_ratios(ratios=ratios)\n",
    "\n",
    "print(\"\\n\u2713 Evaluaci\u00f3n completada\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U-9ogzT35aZ"
   },
   "outputs": [],
   "source": [
    "# Guardar resultados semi-supervised\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"\u2713 Resultados guardados: data/processed/semi_supervised_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSBCuwkd35aZ"
   },
   "outputs": [],
   "source": [
    "# Visualizar comparaci\u00f3n de m\u00e9todos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "colors = {'Supervised Baseline': 'red', 'Label Propagation': 'blue', 'Self-Training': 'green'}\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    # Graficar baseline\n",
    "    baseline_value = results_df[results_df['method'] == 'Supervised Baseline'][metric].values[0]\n",
    "    ax.axhline(y=baseline_value, color='red', linestyle='--', linewidth=2, label='Supervised Baseline')\n",
    "\n",
    "    # Graficar Label Propagation\n",
    "    lp_data = results_df[results_df['method'] == 'Label Propagation']\n",
    "    ax.plot(lp_data['labeled_ratio'] * 100, lp_data[metric], 'bo-', linewidth=2,\n",
    "            markersize=8, label='Label Propagation')\n",
    "\n",
    "    # Graficar Self-Training\n",
    "    st_data = results_df[results_df['method'] == 'Self-Training']\n",
    "    ax.plot(st_data['labeled_ratio'] * 100, st_data[metric], 'gs-', linewidth=2,\n",
    "            markersize=8, label='Self-Training')\n",
    "\n",
    "    ax.set_xlabel('Porcentaje de Datos Etiquetados (%)', fontsize=11)\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Ratio de Labels', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/07_semi_supervised_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CMBGU3K35aa"
   },
   "source": [
    "## 6\ufe0f\u20e3 RESULTADOS Y CONCLUSIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nE_qCvB735aa"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n\u2713 N\u00famero \u00f3ptimo de clusters: {optimal_k}\")\n",
    "print(f\"\\n\ud83d\udcca M\u00e9tricas por algoritmo:\")\n",
    "print(clustering_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI\u00d3N:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. SILHOUETTE SCORE (rango: -1 a 1)\n",
    "   - Mide la similitud de un objeto con su cluster vs otros clusters\n",
    "   - Valores m\u00e1s altos indican mejor separaci\u00f3n\n",
    "   - Interpretaci\u00f3n: > 0.5 (bueno), > 0.7 (excelente)\n",
    "\n",
    "2. DAVIES-BOULDIN INDEX (menor es mejor)\n",
    "   - Raz\u00f3n promedio de similitud intra-cluster vs inter-cluster\n",
    "   - Valores m\u00e1s bajos indican clusters mejor definidos\n",
    "   - Interpretaci\u00f3n: < 1.0 (bueno), < 0.5 (excelente)\n",
    "\n",
    "3. COMPARACI\u00d3N CON RATINGS REALES\n",
    "   - Adjusted Rand Index mide acuerdo entre clustering y ratings\n",
    "   - Rango: -1 a 1 (1 = acuerdo perfecto, 0 = acuerdo aleatorio)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4-dSHmW35aa"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - SEMI-SUPERVISED LEARNING\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI\u00d3N:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. ACCURACY\n",
    "   - Proporci\u00f3n de predicciones correctas\n",
    "   - M\u00e9trica general de rendimiento\n",
    "\n",
    "2. PRECISION\n",
    "   - Proporci\u00f3n de predicciones positivas que fueron correctas\n",
    "   - Importante cuando el costo de falsos positivos es alto\n",
    "\n",
    "3. RECALL\n",
    "   - Proporci\u00f3n de casos positivos que fueron identificados\n",
    "   - Importante cuando el costo de falsos negativos es alto\n",
    "\n",
    "4. F1-SCORE\n",
    "   - Media arm\u00f3nica entre Precision y Recall\n",
    "   - M\u00e9trica equilibrada para clasificaci\u00f3n desbalanceada\n",
    "\n",
    "5. RATIO DE LABELS\n",
    "   - Proporci\u00f3n de datos etiquetados usados en entrenamiento\n",
    "   - Medir el impacto de tener menos datos etiquetados\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYrccMrY35ab"
   },
   "outputs": [],
   "source": [
    "# An\u00e1lisis de clusters vs ratings\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AN\u00c1LISIS DETALLADO: CLUSTERS K-MEANS vs RATINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear DataFrame con resultados (usar solo filas limpias)\n",
    "df_clustered = df.loc[df_clean.index].copy()\n",
    "df_clustered['cluster_kmeans'] = kmeans_labels\n",
    "df_clustered['cluster_agg'] = agg_labels\n",
    "df_clustered['cluster_dbscan'] = dbscan_labels\n",
    "\n",
    "print(\"\\n\ud83d\udcca Distribuci\u00f3n de ratings por cluster K-Means:\")\n",
    "crosstab = pd.crosstab(df_clustered['rating'], df_clustered['cluster_kmeans'], margins=True)\n",
    "print(crosstab)\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Observaciones:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['cluster_kmeans'] == cluster]\n",
    "    rating_dist = cluster_data['rating'].value_counts()\n",
    "    print(f\"  Cluster {cluster}: {len(cluster_data)} cooperativas\")\n",
    "    print(f\"    Distribuci\u00f3n de ratings: {dict(rating_dist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHrZlwqS35ah"
   },
   "outputs": [],
   "source": [
    "# Conclusiones finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSIONES Y RECOMENDACIONES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "\ud83c\udfaf HALLAZGOS PRINCIPALES:\n",
    "\n",
    "1. CLUSTERING NO SUPERVISADO:\n",
    "   \u2022 Se identificaron patrones naturales en los datos financieros\n",
    "   \u2022 El n\u00famero \u00f3ptimo de clusters fue determinado mediante Silhouette Score\n",
    "   \u2022 K-Means proporciona una buena separaci\u00f3n de cooperativas\n",
    "   \u2022 Los clusters muestran cierta coherencia con los ratings reales\n",
    "\n",
    "2. COMPARACI\u00d3N CON RATINGS REALES:\n",
    "   \u2022 Existe una relaci\u00f3n parcial entre clusters y ratings\n",
    "   \u2022 Algunos ratings se distribuyen en m\u00faltiples clusters\n",
    "   \u2022 Sugiere que los indicadores financieros capturan matices no reflejados en ratings simples\n",
    "\n",
    "3. SEMI-SUPERVISED LEARNING:\n",
    "   \u2022 Label Propagation muestra mejor rendimiento con menos datos etiquetados\n",
    "   \u2022 Self-Training es m\u00e1s inestable en ratios bajos\n",
    "   \u2022 Ambos m\u00e9todos se acercan al baseline supervisado con ~50% de datos etiquetados\n",
    "\n",
    "\ud83d\udccc RECOMENDACIONES:\n",
    "\n",
    "   1. Para clasificaci\u00f3n de nuevas cooperativas:\n",
    "      \u2192 Usar modelo supervisado con todos los datos disponibles\n",
    "      \u2192 Si hay nuevas cooperativas sin etiquetar, aplicar Label Propagation\n",
    "\n",
    "   2. Para segmentaci\u00f3n de cooperativas:\n",
    "      \u2192 K-Means proporciona grupos interpretables\n",
    "      \u2192 Validar grupos con expertos en finanzas\n",
    "\n",
    "   3. Mejoras futuras:\n",
    "      \u2192 Incluir m\u00e1s indicadores financieros\n",
    "      \u2192 Validaci\u00f3n cruzada temporal (datos hist\u00f3ricos)\n",
    "      \u2192 An\u00e1lisis de estabilidad de clusters\n",
    "      \u2192 Investigar por qu\u00e9 algunos ratings se distribuyen en m\u00faltiples clusters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vlw03Vhp35ah"
   },
   "outputs": [],
   "source": [
    "# Guardar resultados finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS FINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar datos clustered\n",
    "df_clustered.to_csv('data/processed/cooperativas_clustered.csv', index=False)\n",
    "print(\"\u2713 Datos clustered guardados\")\n",
    "\n",
    "# Guardar m\u00e9tricas\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"\u2713 M\u00e9tricas guardadas\")\n",
    "\n",
    "print(\"\\n\u2705 An\u00e1lisis completado exitosamente\")\n",
    "print(\"\\n\ud83d\udcc1 Archivos generados:\")\n",
    "print(\"  \u2022 data/processed/cooperativas_data.csv\")\n",
    "print(\"  \u2022 data/processed/cooperativas_clustered.csv\")\n",
    "print(\"  \u2022 data/processed/clustering_metrics.csv\")\n",
    "print(\"  \u2022 data/processed/semi_supervised_results.csv\")\n",
    "print(\"  \u2022 figures/01_distribucion_por_rating.png\")\n",
    "print(\"  \u2022 figures/02_matriz_correlacion.png\")\n",
    "print(\"  \u2022 figures/03_tsne_visualization.png\")\n",
    "print(\"  \u2022 figures/04_elbow_analysis.png\")\n",
    "print(\"  \u2022 figures/05_clustering_results_tsne.png\")\n",
    "print(\"  \u2022 figures/06_confusion_matrices.png\")\n",
    "print(\"  \u2022 figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM5PVmRq35ah"
   },
   "source": [
    "## \ud83d\udcda Referencias y Metodolog\u00eda\n",
    "\n",
    "### Fuentes Te\u00f3ricas\n",
    "\n",
    "1. **Clustering No Supervisado:**\n",
    "   - Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory\n",
    "   - Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation of cluster analysis\n",
    "   - Davies, D. L., & Bouldin, D. W. (1979). A cluster separation measure\n",
    "\n",
    "2. **Semi-Supervised Learning:**\n",
    "   - Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Sch\u00f6lkopf, B. (2004). Learning with local and global consistency\n",
    "   - Rosenberg, D., Hebert, M., & Schneiderman, H. (2005). Semi-supervised self-training of object detection models\n",
    "\n",
    "3. **Visualizaci\u00f3n:**\n",
    "   - van der Maaten, L., & Hinton, G. (2008). Visualizing Data using t-SNE\n",
    "\n",
    "### Indicadores Financieros\n",
    "\n",
    "Referencia: Superintendencia de Econom\u00eda Popular y Solidaria (SEPS)\n",
    "- https://www.seps.gob.ec\n",
    "- ASIS: Asociaci\u00f3n de Supervisores de Instituciones de Seguros\n",
    "- https://www.asis.fin.ec\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook generado:** Noviembre 2025\n",
    "\n",
    "**Pr\u00f3ximos pasos:**\n",
    "1. Obtener datos reales de cooperativas (archivos PDF)\n",
    "2. Validar resultados con expertos en finanzas\n",
    "3. Realizar an\u00e1lisis temporal de estabilidad de clusters\n",
    "4. Investigar casos discrepantes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}