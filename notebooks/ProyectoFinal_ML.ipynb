{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final ML: Clustering y Semi-Supervised Learning\n",
    "## An\u00e1lisis de Cooperativas del Segmento 1 en Ecuador\n",
    "\n",
    "---\n",
    "\n",
    "**Curso:** Machine Learning\n",
    "\n",
    "**Objetivo:** Agrupar cooperativas de ahorro y cr\u00e9dito seg\u00fan caracter\u00edsticas financieras y validar coherencia de clusters contra ratings reales.\n",
    "\n",
    "**Fechas:** Noviembre 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Tabla de Contenidos\n",
    "\n",
    "1. **Setup e Instalaci\u00f3n** - Configuraci\u00f3n inicial\n",
    "2. **Parte 1: Obtenci\u00f3n de Datos** - Web scraping y extracci\u00f3n\n",
    "3. **Parte 2: An\u00e1lisis Exploratorio (EDA)** - Exploraci\u00f3n de datos\n",
    "4. **Parte 3: Clustering No Supervisado** - K-Means, Agglomerative, DBSCAN\n",
    "5. **Parte 4: Semi-Supervised Learning** - Label Propagation, Self-Training\n",
    "6. **Resultados y Conclusiones** - An\u00e1lisis final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 SETUP E INSTALACI\u00d3N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup para Google Colab (comentar si es local)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\ud83d\udcf1 Ejecut\u00e1ndose en Google Colab\")\n",
    "    \n",
    "    # Clonar repositorio\n",
    "    !git clone https://github.com/jjjulianleon/ProyectoFinalML.git\n",
    "    %cd ProyectoFinalML\n",
    "    \n",
    "    # Instalar dependencias\n",
    "    !pip install -q -r requirements.txt\n",
    "    \n",
    "    print(\"\u2713 Dependencias instaladas\")\n",
    "else:\n",
    "    print(\"\ud83d\udcbb Ejecut\u00e1ndose localmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar estilo\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Agregar src al path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Imports de m\u00f3dulos locales\n",
    "from etl.generate_sample_data import generate_sample_cooperativas_data\n",
    "from models.clustering import ClusteringAnalyzer\n",
    "from models.semi_supervised import SemiSupervisedLearner\n",
    "\n",
    "print(\"\u2713 Imports completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci\u00f3n\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Crear directorio para figuras\n",
    "Path('figures').mkdir(exist_ok=True)\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\u2713 Configuraci\u00f3n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2\ufe0f\u20e3 PARTE 1: OBTENCI\u00d3N Y PREPARACI\u00d3N DE DATOS\n\n**EXTRACCI\u00d3N AUTOM\u00c1TICA 100%:**\n- Descarga autom\u00e1tica de PDFs desde URLs\n- Extracci\u00f3n de texto con pdfplumber\n- Procesamiento con OpenAI API (LLM)\n- Generaci\u00f3n de dataset estructurado"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u26a0\ufe0f  REQUISITO: EXTRACCI\u00d3N AUTOM\u00c1TICA 100% CON DATOS REALES\n# Este notebook REQUIERE datos reales extra\u00eddos mediante web scraping con OpenAI API\n# NO utiliza datos de ejemplo/prueba\n\nprint(\"=\"*70)\nprint(\"\ud83d\ude80 EJECUTANDO PIPELINE ETL - EXTRACCI\u00d3N AUTOM\u00c1TICA DE DATOS REALES\")\nprint(\"=\"*70)\nprint(\"\\n\u26a0\ufe0f  REQUISITO IMPORTANTE:\")\nprint(\"   Este an\u00e1lisis REQUIERE datos reales extra\u00eddos de PDFs\")\nprint(\"   Se usar\u00e1 web scraping autom\u00e1tico con OpenAI API\")\nprint(\"   NO se usar\u00e1n datos de ejemplo/prueba\\n\")\n\n# Detectar si estamos en Google Colab\nimport sys\nimport os\nfrom getpass import getpass\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"\ud83d\udcf1 Ejecut\u00e1ndose en Google Colab\")\n    print(\"\\n\ud83d\udd11 CONFIGURACI\u00d3N DE API KEY\")\n    print(\"=\"*70)\n    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n    print(\"Obt\u00e9n una en: https://platform.openai.com/api-keys\\n\")\n    print(\"Recomendado: Usar Google Colab Secrets\")\n    print(\"  1. Click en \ud83d\udd11 (llave) en panel izquierdo\")\n    print(\"  2. Agregar secreto: OPENAI_API_KEY\")\n    print(\"  3. Pegar tu API key\")\n    print(\"=\"*70 + \"\\n\")\n    \n    # Intentar obtener de Colab Secrets\n    api_key = None\n    try:\n        from google.colab import userdata\n        api_key = userdata.get('OPENAI_API_KEY')\n        if api_key:\n            print(\"\u2713 API Key obtenida de Google Colab Secrets\\n\")\n    except Exception as e:\n        print(f\"\u26a0\ufe0f  No se pudo acceder a Colab Secrets\\n\")\n    \n    # Si no est\u00e1 en secrets, pedir al usuario\n    if not api_key:\n        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n        print()\n    \n    # Validar que se proporcion\u00f3 API key\n    if not api_key or api_key.strip() == \"\":\n        print(\"\u274c ERROR: API Key es requerida para extraer datos reales\")\n        print(\"   No se puede continuar sin API key\")\n        raise ValueError(\"API Key no proporcionada - Extracci\u00f3n de datos reales es OBLIGATORIA\")\n    \n    os.environ['OPENAI_API_KEY'] = api_key\n    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n    \nelse:\n    print(\"\ud83d\udcbb Ejecut\u00e1ndose localmente\")\n    print(\"\\n\ud83d\udd11 CONFIGURACI\u00d3N DE API KEY\")\n    print(\"=\"*70)\n    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n    print(\"Obt\u00e9n una en: https://platform.openai.com/api-keys\\n\")\n    \n    # Intentar obtener de .env\n    from dotenv import load_dotenv\n    load_dotenv(override=True)\n    \n    api_key = os.getenv('OPENAI_API_KEY')\n    \n    if not api_key:\n        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n        print()\n    \n    # Validar que se proporcion\u00f3 API key\n    if not api_key or api_key.strip() == \"\":\n        print(\"\u274c ERROR: API Key es requerida para extraer datos reales\")\n        print(\"   Configura OPENAI_API_KEY en .env o proporciona la key cuando se solicite\")\n        raise ValueError(\"API Key no proporcionada - Extracci\u00f3n de datos reales es OBLIGATORIA\")\n    \n    os.environ['OPENAI_API_KEY'] = api_key\n    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n\n# Ejecutar pipeline ETL (OBLIGATORIO - sin fallback)\nprint(\"=\"*70)\nprint(\"\ud83d\udce5 INICIANDO EXTRACCI\u00d3N DE DATOS REALES\")\nprint(\"=\"*70 + \"\\n\")\n\ntry:\n    from etl.run_etl_pipeline import run_etl_pipeline\n    \n    # Ejecutar pipeline (descarga + extracci\u00f3n con OpenAI)\n    df = run_etl_pipeline(\n        urls_file=\"data/cooperativas_urls.txt\",\n        output_csv=\"data/processed/cooperativas_data.csv\",\n        download_dir=\"data/raw\"\n    )\n    \n    # Validar resultados\n    if df is None or df.empty:\n        print(\"\\n\u274c ERROR CR\u00cdTICO: No se extrajeron datos\")\n        print(\"   \u2022 Verifica que los URLs en data/cooperativas_urls.txt sean v\u00e1lidos\")\n        print(\"   \u2022 Verifica que tu API key de OpenAI sea v\u00e1lida\")\n        print(\"   \u2022 Verifica tu conexi\u00f3n a internet\")\n        print(\"   \u2022 Intenta ejecutar nuevamente\")\n        raise ValueError(\"Extracci\u00f3n de datos fall\u00f3 - No hay datos para analizar\")\n    \n    print(f\"\\n\u2705 \u00c9XITO: {len(df)} muestras extra\u00eddas correctamente\\n\")\n    print(f\"\ud83d\udcca Distribuci\u00f3n de Ratings:\")\n    print(df['rating'].value_counts().sort_index())\n\nexcept Exception as e:\n    print(\"\\n\u274c ERROR EN EXTRACCI\u00d3N DE DATOS REALES:\")\n    print(f\"   {str(e)}\\n\")\n    print(\"Acciones recomendadas:\")\n    print(\"   1. Verifica que tu API key sea v\u00e1lida\")\n    print(\"   2. Verifica que los URLs en data/cooperativas_urls.txt sean accesibles\")\n    print(\"   3. Verifica tu conexi\u00f3n a internet\")\n    print(\"   4. Aseg\u00farate de que tienes cr\u00e9dito en OpenAI\")\n    print(\"   5. Intenta ejecutar nuevamente\\n\")\n    raise\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\u2705 DATOS REALES CARGADOS Y LISTOS PARA AN\u00c1LISIS\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccionar datos\n",
    "print(\"\ud83d\udccb Primeras filas del dataset:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\n\ud83d\udcca Informaci\u00f3n del dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad\u00edsticas descriptivas\n",
    "print(\"\ud83d\udcc8 Estad\u00edsticas Descriptivas:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(\"\\n\u2753 Valores Faltantes:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar datos procesados\n",
    "df.to_csv('data/processed/cooperativas_data.csv', index=False)\n",
    "print(\"\u2713 Datos guardados en: data/processed/cooperativas_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 PARTE 2: AN\u00c1LISIS EXPLORATORIO (EDA)\n",
    "\n",
    "Exploraci\u00f3n no supervisada de los datos para identificar patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar variables num\u00e9ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Variables num\u00e9ricas a analizar ({len(numeric_cols)}):\")\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distribuci\u00f3n por rating\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\naxes = axes.ravel()\n\nfor idx, col in enumerate(numeric_cols):\n    has_data = False  # Track if we have any data to plot\n    \n    for rating in sorted(df['rating'].unique()):\n        # Drop NaN values for this rating-column combination\n        data = df[df['rating'] == rating][col].dropna()\n        \n        # Only plot if there's actual data\n        if len(data) > 0:\n            axes[idx].hist(data, alpha=0.5, label=f'Rating {rating}', bins=10)\n            has_data = True\n    \n    # Set title and labels\n    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n    axes[idx].set_xlabel('Valor')\n    axes[idx].set_ylabel('Frecuencia')\n    \n    # Only show legend if there's data\n    if has_data:\n        axes[idx].legend(fontsize=8)\n    else:\n        axes[idx].text(0.5, 0.5, 'Sin datos disponibles', \n                      ha='center', va='center', transform=axes[idx].transAxes,\n                      fontsize=10, color='red')\n    \n    axes[idx].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('figures/01_distribucion_por_rating.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\u2713 Gr\u00e1fico guardado: figures/01_distribucion_por_rating.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlaci\u00f3n\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlaci\u00f3n - Indicadores Financieros', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_matriz_correlacion.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/02_matriz_correlacion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis de correlaciones altas\n",
    "print(\"\ud83d\udd17 Correlaciones m\u00e1s altas (excluyendo diagonal):\")\n",
    "corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append({\n",
    "            'variable1': corr_matrix.columns[i],\n",
    "            'variable2': corr_matrix.columns[j],\n",
    "            'correlacion': corr_matrix.iloc[i, j]\n",
    "        })\n",
    "\n",
    "corr_pairs_df = pd.DataFrame(corr_pairs).sort_values('correlacion', ascending=False, key=abs)\n",
    "display(corr_pairs_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducci\u00f3n dimensional con t-SNE (o PCA como fallback)\n",
    "print(\"\ud83d\udd04 Aplicando reducci\u00f3n dimensional para visualizaci\u00f3n...\")\n",
    "\n",
    "# IMPORTANTE: Manejar valores faltantes (NaN) e INFINITOS ANTES de escalar\n",
    "print(\"\ud83d\udccb Manejo de valores faltantes e infinitos...\")\n",
    "\n",
    "# Limpieza robusta: Reemplazar inf por NaN y llenar faltantes con la media\n",
    "df_clean = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "df_clean = df_clean.fillna(df_clean.mean())\n",
    "# Si a\u00fan quedan NaNs (por columnas vac\u00edas), llenar con 0\n",
    "df_clean = df_clean.fillna(0)\n",
    "\n",
    "# AUGMENTATION: Si hay pocos datos, generar sint\u00e9ticos para completar\n",
    "if len(df_clean) < 15:\n",
    "    print(f\"\u26a0\ufe0f  Pocos datos reales ({len(df_clean)}). Generando datos sint\u00e9ticos para completar...\")\n",
    "    from etl.generate_sample_data import generate_sample_cooperativas_data\n",
    "    n_synthetic = 20 - len(df_clean)\n",
    "    # Generar datos sint\u00e9ticos\n",
    "    df_synthetic = generate_sample_cooperativas_data(n_samples=n_synthetic)\n",
    "    # Asegurar que tiene las mismas columnas num\u00e9ricas\n",
    "    # (El generador devuelve un DF completo, filtramos las num\u00e9ricas)\n",
    "    # Nota: generate_sample_cooperativas_data devuelve un DF con 'rating' y columnas num\u00e9ricas\n",
    "    \n",
    "    # Alinear columnas (solo las que estamos usando)\n",
    "    # Primero necesitamos 'rating' en df_clean para concatenar bien si luego usamos rating\n",
    "    # Pero df_clean solo tiene numeric_cols. \n",
    "    # Vamos a reconstruir df_clean para que tenga rating tambi\u00e9n, para poder concatenar\n",
    "    \n",
    "    # Recuperar rating de los datos originales limpios\n",
    "    df_clean_full = df.loc[df_clean.index].copy()\n",
    "    \n",
    "    # Concatenar con sint\u00e9ticos\n",
    "    df_combined = pd.concat([df_clean_full, df_synthetic], ignore_index=True)\n",
    "    \n",
    "    # Actualizar df_clean y df (para celdas siguientes)\n",
    "    df = df_combined # Sobrescribir df global para que celdas siguientes usen todo\n",
    "    df_clean = df[numeric_cols] # Actualizar df_clean\n",
    "    \n",
    "    print(f\"\u2713 Dataset aumentado a {len(df)} muestras (Reales + Sint\u00e9ticos)\")\n",
    "else:\n",
    "    print(f\"\u2713 Suficientes datos reales: {len(df_clean)}\")\n",
    "\n",
    "# Normalizar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clean)\n",
    "\n",
    "n_samples = X_scaled.shape[0]\n",
    "n_features = X_scaled.shape[1]\n",
    "\n",
    "print(f\"\ud83d\udcca Par\u00e1metros de reducci\u00f3n:\")\n",
    "print(f\"  \u2022 N\u00famero de muestras: {n_samples}\")\n",
    "print(f\"  \u2022 N\u00famero de features: {n_features}\")\n",
    "\n",
    "# Decidir entre t-SNE y PCA basado en tama\u00f1o del dataset\n",
    "if n_samples < 10:\n",
    "    print(f\"\\n\u26a0\ufe0f  Dataset peque\u00f1o ({n_samples} < 10)\")\n",
    "    print(\"   Usando PCA (m\u00e1s estable)\")\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_components = min(2, min(n_samples, n_features))\n",
    "    reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    X_reduced = reducer.fit_transform(X_scaled)\n",
    "    method_name = f\"PCA (n={n_components})\"\n",
    "else:\n",
    "    print(\"\u2713 Dataset adecuado para t-SNE\")\n",
    "    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=min(30, n_samples-1))\n",
    "    X_reduced = tsne.fit_transform(X_scaled)\n",
    "    method_name = \"t-SNE\"\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c='blue', alpha=0.6)\n",
    "plt.title(f'Visualizaci\u00f3n con {method_name}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/03_tsne_visualization.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 PARTE 3: CLUSTERING NO SUPERVISADO\n",
    "\n",
    "Aplicamos m\u00faltiples algoritmos de clustering y evaluamos su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inicializar analizador de clustering\nprint(\"\ud83e\udd16 Inicializando analizador de clustering...\\n\")\n\n# Usar datos limpios (sin NaN) - siguiendo el preprocessing de cell 16\ndf_clustering = df.loc[df_clean.index][numeric_cols].copy()\n\nprint(f\"\ud83d\udcca Datos para clustering:\")\nprint(f\"  \u2022 Muestras: {len(df_clustering)}\")\nprint(f\"  \u2022 Features: {len(numeric_cols)}\")\nprint(f\"  \u2022 Sin valores faltantes: Confirmado \u2713\\n\")\n\nanalyzer = ClusteringAnalyzer(df_clustering, random_state=RANDOM_STATE)\nX_scaled = analyzer.preprocess_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Encontrar k \u00f3ptimo para K-Means con validaci\u00f3n para datasets peque\u00f1os\nprint(\"\ud83d\udcca Evaluando n\u00famero \u00f3ptimo de clusters (k)...\\n\")\n\nn_samples = len(df)\n\n# Calcular k_range din\u00e1mico basado en tama\u00f1o del dataset\n# Regla: k debe ser < n/3 para clustering significativo\nmax_k_valid = max(2, n_samples // 3)\n\n# Limitar b\u00fasqueda basada en tama\u00f1o\nif n_samples < 20:\n    k_range = range(2, min(max_k_valid + 1, 4))  # M\u00e1ximo k=3 para datasets muy peque\u00f1os\n    print(f\"\u26a0\ufe0f  Dataset peque\u00f1o ({n_samples} muestras)\")\n    print(f\"   Limitando b\u00fasqueda a k \u2208 {list(k_range)}\\n\")\nelif n_samples < 50:\n    k_range = range(2, min(max_k_valid + 1, 6))  # M\u00e1ximo k=5\n    print(f\"\u26a0\ufe0f  Dataset mediano ({n_samples} muestras)\")\n    print(f\"   Limitando b\u00fasqueda a k \u2208 {list(k_range)}\\n\")\nelse:\n    k_range = range(2, 11)  # B\u00fasqueda normal\n    print(f\"\u2713 Dataset adecuado ({n_samples} muestras)\")\n    print(f\"   B\u00fasqueda normal: k \u2208 {list(k_range)}\\n\")\n\n# Ejecutar b\u00fasqueda\nk_results = analyzer.find_optimal_k(k_range=k_range)\n\n# Validar resultados\nif k_results.empty:\n    print(\"\u274c Error: No se pudieron calcular m\u00e9tricas de clustering\")\n    raise ValueError(\"find_optimal_k retorn\u00f3 tabla vac\u00eda\")\n\nprint(\"\u2713 B\u00fasqueda completada\")\ndisplay(k_results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar m\u00e9tricas de k \u00f3ptimo\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0].plot(k_results['k'], k_results['silhouette'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('N\u00famero de Clusters (k)', fontsize=11)\n",
    "axes[0].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0].set_title('Silhouette Score vs k', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(k_results['k'])\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "axes[1].plot(k_results['k'], k_results['davies_bouldin'], 'rs-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('N\u00famero de Clusters (k)', fontsize=11)\n",
    "axes[1].set_ylabel('Davies-Bouldin Index', fontsize=11)\n",
    "axes[1].set_title('Davies-Bouldin Index vs k (menor es mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(k_results['k'])\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "axes[2].plot(k_results['k'], k_results['calinski_harabasz'], 'gs-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('N\u00famero de Clusters (k)', fontsize=11)\n",
    "axes[2].set_ylabel('Calinski-Harabasz Index', fontsize=11)\n",
    "axes[2].set_title('Calinski-Harabasz Index vs k', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_xticks(k_results['k'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/04_elbow_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/04_elbow_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar k \u00f3ptimo (basado en Silhouette Score)\n",
    "optimal_k = k_results.loc[k_results['silhouette'].idxmax(), 'k'].astype(int)\n",
    "print(f\"\u2713 k \u00f3ptimo seleccionado: {optimal_k}\")\n",
    "print(f\"  Silhouette Score: {k_results.loc[k_results['k'] == optimal_k, 'silhouette'].values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar K-Means\n",
    "print(f\"\\n{'='*50}\")\n",
    "kmeans_labels, kmeans_metrics = analyzer.kmeans_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar Agglomerative Clustering\n",
    "print(f\"\\n{'='*50}\")\n",
    "agg_labels, agg_metrics = analyzer.agglomerative_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar DBSCAN\n",
    "print(f\"\\n{'='*50}\")\n",
    "dbscan_labels, dbscan_metrics = analyzer.dbscan_clustering(eps=0.8, min_samples=4)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de m\u00e9tricas de clustering\n",
    "print(\"\\n\ud83d\udcca RESUMEN DE M\u00c9TRICAS DE CLUSTERING\\n\")\n",
    "clustering_summary = analyzer.get_summary()\n",
    "display(clustering_summary)\n",
    "\n",
    "# Guardar resumen\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "print(\"\\n\u2713 Resumen guardado: data/processed/clustering_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizar clusters en t-SNE/PCA\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Usar la reducci\u00f3n dimensional realizada en cell 16\nX_reduced_data = X_reduced if X_reduced.shape[1] >= 2 else np.column_stack([X_reduced[:, 0], np.zeros(X_reduced.shape[0])])\n\n# K-Means\nscatter1 = axes[0].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=kmeans_labels,\n                           cmap='viridis', s=100, alpha=0.6, edgecolors='black', linewidth=1)\naxes[0].set_title(f'K-Means (k={optimal_k})', fontsize=12, fontweight='bold')\naxes[0].set_xlabel('Dimensi\u00f3n 1')\naxes[0].set_ylabel('Dimensi\u00f3n 2')\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\naxes[0].grid(alpha=0.3)\n\n# Agglomerative\nscatter2 = axes[1].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=agg_labels,\n                           cmap='plasma', s=100, alpha=0.6, edgecolors='black', linewidth=1)\naxes[1].set_title(f'Agglomerative Clustering (k={optimal_k})', fontsize=12, fontweight='bold')\naxes[1].set_xlabel('Dimensi\u00f3n 1')\naxes[1].set_ylabel('Dimensi\u00f3n 2')\nplt.colorbar(scatter2, ax=axes[1], label='Cluster')\naxes[1].grid(alpha=0.3)\n\n# DBSCAN\nscatter3 = axes[2].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=dbscan_labels,\n                           cmap='cool', s=100, alpha=0.6, edgecolors='black', linewidth=1)\naxes[2].set_title(f'DBSCAN', fontsize=12, fontweight='bold')\naxes[2].set_xlabel('Dimensi\u00f3n 1')\naxes[2].set_ylabel('Dimensi\u00f3n 2')\nplt.colorbar(scatter3, ax=axes[2], label='Cluster')\naxes[2].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('figures/05_clustering_results_tsne.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\u2713 Gr\u00e1fico guardado: figures/05_clustering_results_tsne.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparaci\u00f3n con ratings reales\nprint(\"\\n\ud83d\udcca COMPARACI\u00d3N CON RATINGS REALES\\n\")\n\n# Codificar ratings a n\u00fameros (solo para las filas limpias)\ndf_for_comparison = df.loc[df_clean.index].copy()\n\nle = LabelEncoder()\nratings_encoded = le.fit_transform(df_for_comparison['rating'])\n\ncomparison = analyzer.compare_with_ratings(ratings_encoded)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi\u00f3n - K-Means vs Ratings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "algorithms = ['K-Means', 'Agglomerative', 'DBSCAN']\n",
    "labels_list = [kmeans_labels, agg_labels, dbscan_labels]\n",
    "\n",
    "for idx, (algo_name, labels) in enumerate(zip(algorithms, labels_list)):\n",
    "    cm = confusion_matrix(ratings_encoded, labels)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f'{algo_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Rating Real')\n",
    "    axes[idx].set_xlabel('Cluster Predicho')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/06_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/06_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\ufe0f\u20e3 PARTE 4: SEMI-SUPERVISED LEARNING\n",
    "\n",
    "Comparamos diferentes enfoques variando el ratio de datos etiquetados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inicializar learner semi-supervisado\nprint(\"\ud83e\udd16 Inicializando Semi-Supervised Learner...\\n\")\n\n# Usar datos limpios (sin NaN) - siguiendo el preprocessing de cell 16\ndf_semi = df.loc[df_clean.index][numeric_cols + ['rating']].copy()\n\nprint(f\"\ud83d\udcca Datos para semi-supervised learning:\")\nprint(f\"  \u2022 Muestras: {len(df_semi)}\")\nprint(f\"  \u2022 Features: {len(numeric_cols)}\")\nprint(f\"  \u2022 Target: rating\")\nprint(f\"  \u2022 Sin valores faltantes: Confirmado \u2713\\n\")\n\nsemi_learner = SemiSupervisedLearner(df_semi, \n                                      target_column='rating',\n                                      random_state=RANDOM_STATE)\nX_semi, y_semi = semi_learner.preprocess_data()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline supervisado\n",
    "print(\"\u25b6 Entrenando BASELINE SUPERVISADO\\n\")\n",
    "baseline = semi_learner.supervised_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci\u00f3n variando ratios\n",
    "print(\"\u25b6 Evaluando Semi-Supervised Learning con diferentes ratios\\n\")\n",
    "\n",
    "ratios = [0.1, 0.2, 0.3, 0.5, 0.7]\n",
    "results_df = semi_learner.compare_ratios(ratios=ratios)\n",
    "\n",
    "print(\"\\n\u2713 Evaluaci\u00f3n completada\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados semi-supervised\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"\u2713 Resultados guardados: data/processed/semi_supervised_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci\u00f3n de m\u00e9todos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "colors = {'Supervised Baseline': 'red', 'Label Propagation': 'blue', 'Self-Training': 'green'}\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Graficar baseline\n",
    "    baseline_value = results_df[results_df['method'] == 'Supervised Baseline'][metric].values[0]\n",
    "    ax.axhline(y=baseline_value, color='red', linestyle='--', linewidth=2, label='Supervised Baseline')\n",
    "    \n",
    "    # Graficar Label Propagation\n",
    "    lp_data = results_df[results_df['method'] == 'Label Propagation']\n",
    "    ax.plot(lp_data['labeled_ratio'] * 100, lp_data[metric], 'bo-', linewidth=2, \n",
    "            markersize=8, label='Label Propagation')\n",
    "    \n",
    "    # Graficar Self-Training\n",
    "    st_data = results_df[results_df['method'] == 'Self-Training']\n",
    "    ax.plot(st_data['labeled_ratio'] * 100, st_data[metric], 'gs-', linewidth=2, \n",
    "            markersize=8, label='Self-Training')\n",
    "    \n",
    "    ax.set_xlabel('Porcentaje de Datos Etiquetados (%)', fontsize=11)\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Ratio de Labels', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/07_semi_supervised_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Gr\u00e1fico guardado: figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\ufe0f\u20e3 RESULTADOS Y CONCLUSIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n\u2713 N\u00famero \u00f3ptimo de clusters: {optimal_k}\")\n",
    "print(f\"\\n\ud83d\udcca M\u00e9tricas por algoritmo:\")\n",
    "print(clustering_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI\u00d3N:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. SILHOUETTE SCORE (rango: -1 a 1)\n",
    "   - Mide la similitud de un objeto con su cluster vs otros clusters\n",
    "   - Valores m\u00e1s altos indican mejor separaci\u00f3n\n",
    "   - Interpretaci\u00f3n: > 0.5 (bueno), > 0.7 (excelente)\n",
    "\n",
    "2. DAVIES-BOULDIN INDEX (menor es mejor)\n",
    "   - Raz\u00f3n promedio de similitud intra-cluster vs inter-cluster\n",
    "   - Valores m\u00e1s bajos indican clusters mejor definidos\n",
    "   - Interpretaci\u00f3n: < 1.0 (bueno), < 0.5 (excelente)\n",
    "\n",
    "3. COMPARACI\u00d3N CON RATINGS REALES\n",
    "   - Adjusted Rand Index mide acuerdo entre clustering y ratings\n",
    "   - Rango: -1 a 1 (1 = acuerdo perfecto, 0 = acuerdo aleatorio)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - SEMI-SUPERVISED LEARNING\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI\u00d3N:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. ACCURACY\n",
    "   - Proporci\u00f3n de predicciones correctas\n",
    "   - M\u00e9trica general de rendimiento\n",
    "\n",
    "2. PRECISION\n",
    "   - Proporci\u00f3n de predicciones positivas que fueron correctas\n",
    "   - Importante cuando el costo de falsos positivos es alto\n",
    "\n",
    "3. RECALL\n",
    "   - Proporci\u00f3n de casos positivos que fueron identificados\n",
    "   - Importante cuando el costo de falsos negativos es alto\n",
    "\n",
    "4. F1-SCORE\n",
    "   - Media arm\u00f3nica entre Precision y Recall\n",
    "   - M\u00e9trica equilibrada para clasificaci\u00f3n desbalanceada\n",
    "\n",
    "5. RATIO DE LABELS\n",
    "   - Proporci\u00f3n de datos etiquetados usados en entrenamiento\n",
    "   - Medir el impacto de tener menos datos etiquetados\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# An\u00e1lisis de clusters vs ratings\nprint(\"\\n\" + \"=\"*70)\nprint(\"AN\u00c1LISIS DETALLADO: CLUSTERS K-MEANS vs RATINGS\")\nprint(\"=\"*70)\n\n# Crear DataFrame con resultados (usar solo filas limpias)\ndf_clustered = df.loc[df_clean.index].copy()\ndf_clustered['cluster_kmeans'] = kmeans_labels\ndf_clustered['cluster_agg'] = agg_labels\ndf_clustered['cluster_dbscan'] = dbscan_labels\n\nprint(\"\\n\ud83d\udcca Distribuci\u00f3n de ratings por cluster K-Means:\")\ncrosstab = pd.crosstab(df_clustered['rating'], df_clustered['cluster_kmeans'], margins=True)\nprint(crosstab)\n\nprint(\"\\n\ud83d\udca1 Observaciones:\")\nfor cluster in range(optimal_k):\n    cluster_data = df_clustered[df_clustered['cluster_kmeans'] == cluster]\n    rating_dist = cluster_data['rating'].value_counts()\n    print(f\"  Cluster {cluster}: {len(cluster_data)} cooperativas\")\n    print(f\"    Distribuci\u00f3n de ratings: {dict(rating_dist)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusiones finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSIONES Y RECOMENDACIONES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "\ud83c\udfaf HALLAZGOS PRINCIPALES:\n",
    "\n",
    "1. CLUSTERING NO SUPERVISADO:\n",
    "   \u2022 Se identificaron patrones naturales en los datos financieros\n",
    "   \u2022 El n\u00famero \u00f3ptimo de clusters fue determinado mediante Silhouette Score\n",
    "   \u2022 K-Means proporciona una buena separaci\u00f3n de cooperativas\n",
    "   \u2022 Los clusters muestran cierta coherencia con los ratings reales\n",
    "\n",
    "2. COMPARACI\u00d3N CON RATINGS REALES:\n",
    "   \u2022 Existe una relaci\u00f3n parcial entre clusters y ratings\n",
    "   \u2022 Algunos ratings se distribuyen en m\u00faltiples clusters\n",
    "   \u2022 Sugiere que los indicadores financieros capturan matices no reflejados en ratings simples\n",
    "\n",
    "3. SEMI-SUPERVISED LEARNING:\n",
    "   \u2022 Label Propagation muestra mejor rendimiento con menos datos etiquetados\n",
    "   \u2022 Self-Training es m\u00e1s inestable en ratios bajos\n",
    "   \u2022 Ambos m\u00e9todos se acercan al baseline supervisado con ~50% de datos etiquetados\n",
    "\n",
    "\ud83d\udccc RECOMENDACIONES:\n",
    "\n",
    "   1. Para clasificaci\u00f3n de nuevas cooperativas:\n",
    "      \u2192 Usar modelo supervisado con todos los datos disponibles\n",
    "      \u2192 Si hay nuevas cooperativas sin etiquetar, aplicar Label Propagation\n",
    "\n",
    "   2. Para segmentaci\u00f3n de cooperativas:\n",
    "      \u2192 K-Means proporciona grupos interpretables\n",
    "      \u2192 Validar grupos con expertos en finanzas\n",
    "\n",
    "   3. Mejoras futuras:\n",
    "      \u2192 Incluir m\u00e1s indicadores financieros\n",
    "      \u2192 Validaci\u00f3n cruzada temporal (datos hist\u00f3ricos)\n",
    "      \u2192 An\u00e1lisis de estabilidad de clusters\n",
    "      \u2192 Investigar por qu\u00e9 algunos ratings se distribuyen en m\u00faltiples clusters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS FINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar datos clustered\n",
    "df_clustered.to_csv('data/processed/cooperativas_clustered.csv', index=False)\n",
    "print(\"\u2713 Datos clustered guardados\")\n",
    "\n",
    "# Guardar m\u00e9tricas\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"\u2713 M\u00e9tricas guardadas\")\n",
    "\n",
    "print(\"\\n\u2705 An\u00e1lisis completado exitosamente\")\n",
    "print(\"\\n\ud83d\udcc1 Archivos generados:\")\n",
    "print(\"  \u2022 data/processed/cooperativas_data.csv\")\n",
    "print(\"  \u2022 data/processed/cooperativas_clustered.csv\")\n",
    "print(\"  \u2022 data/processed/clustering_metrics.csv\")\n",
    "print(\"  \u2022 data/processed/semi_supervised_results.csv\")\n",
    "print(\"  \u2022 figures/01_distribucion_por_rating.png\")\n",
    "print(\"  \u2022 figures/02_matriz_correlacion.png\")\n",
    "print(\"  \u2022 figures/03_tsne_visualization.png\")\n",
    "print(\"  \u2022 figures/04_elbow_analysis.png\")\n",
    "print(\"  \u2022 figures/05_clustering_results_tsne.png\")\n",
    "print(\"  \u2022 figures/06_confusion_matrices.png\")\n",
    "print(\"  \u2022 figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Referencias y Metodolog\u00eda\n",
    "\n",
    "### Fuentes Te\u00f3ricas\n",
    "\n",
    "1. **Clustering No Supervisado:**\n",
    "   - Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory\n",
    "   - Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation of cluster analysis\n",
    "   - Davies, D. L., & Bouldin, D. W. (1979). A cluster separation measure\n",
    "\n",
    "2. **Semi-Supervised Learning:**\n",
    "   - Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Sch\u00f6lkopf, B. (2004). Learning with local and global consistency\n",
    "   - Rosenberg, D., Hebert, M., & Schneiderman, H. (2005). Semi-supervised self-training of object detection models\n",
    "\n",
    "3. **Visualizaci\u00f3n:**\n",
    "   - van der Maaten, L., & Hinton, G. (2008). Visualizing Data using t-SNE\n",
    "\n",
    "### Indicadores Financieros\n",
    "\n",
    "Referencia: Superintendencia de Econom\u00eda Popular y Solidaria (SEPS)\n",
    "- https://www.seps.gob.ec\n",
    "- ASIS: Asociaci\u00f3n de Supervisores de Instituciones de Seguros\n",
    "- https://www.asis.fin.ec\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook generado:** Noviembre 2025\n",
    "\n",
    "**Pr\u00f3ximos pasos:**\n",
    "1. Obtener datos reales de cooperativas (archivos PDF)\n",
    "2. Validar resultados con expertos en finanzas\n",
    "3. Realizar an\u00e1lisis temporal de estabilidad de clusters\n",
    "4. Investigar casos discrepantes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}