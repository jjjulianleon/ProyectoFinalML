{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yefPHAb735aD"
   },
   "source": [
    "# Proyecto Final ML: Clustering y Semi-Supervised Learning\n",
    "## An√°lisis de Cooperativas del Segmento 1 en Ecuador\n",
    "\n",
    "---\n",
    "\n",
    "**Curso:** Machine Learning\n",
    "\n",
    "**Objetivo:** Agrupar cooperativas de ahorro y cr√©dito seg√∫n caracter√≠sticas financieras y validar coherencia de clusters contra ratings reales.\n",
    "\n",
    "**Fechas:** Noviembre 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqyAvOwS35aG"
   },
   "source": [
    "## üìã Tabla de Contenidos\n",
    "\n",
    "1. **Setup e Instalaci√≥n** - Configuraci√≥n inicial\n",
    "2. **Parte 1: Obtenci√≥n de Datos** - Web scraping y extracci√≥n\n",
    "3. **Parte 2: An√°lisis Exploratorio (EDA)** - Exploraci√≥n de datos\n",
    "4. **Parte 3: Clustering No Supervisado** - K-Means, Agglomerative, DBSCAN\n",
    "5. **Parte 4: Semi-Supervised Learning** - Label Propagation, Self-Training\n",
    "6. **Resultados y Conclusiones** - An√°lisis final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLVvF6mO35aH"
   },
   "source": [
    "## 1Ô∏è‚É£ SETUP E INSTALACI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O52H9c3v35aI"
   },
   "outputs": [],
   "source": [
    "# Setup para Google Colab (comentar si es local)\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üì± Ejecut√°ndose en Google Colab\")\n",
    "\n",
    "    # Clonar repositorio\n",
    "    !git clone https://github.com/jjjulianleon/ProyectoFinalML.git\n",
    "    %cd ProyectoFinalML\n",
    "\n",
    "    # Instalar dependencias\n",
    "    !pip install -q -r requirements.txt\n",
    "\n",
    "    print(\"‚úì Dependencias instaladas\")\n",
    "else:\n",
    "    print(\"üíª Ejecut√°ndose localmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wGhr2K235aJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuraci√≥n de Rutas (Robusta para Local y Colab)\n",
    "current_dir = os.getcwd()\n",
    "print(f\"üìÇ Directorio actual: {current_dir}\")\n",
    "\n",
    "# Determinar la ra√≠z del proyecto\n",
    "if current_dir.endswith('notebooks'):\n",
    "    # Si estamos en notebooks/, subir un nivel\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "else:\n",
    "    # Asumir que estamos en la ra√≠z o en Colab (donde se clona en root)\n",
    "    project_root = current_dir\n",
    "\n",
    "print(f\"üìÇ Ra√≠z del proyecto: {project_root}\")\n",
    "\n",
    "# Agregar 'src' al path para poder importar m√≥dulos\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"‚úì Agregado al path: {src_path}\")\n",
    "\n",
    "# Cargar variables de entorno\n",
    "env_path = os.path.join(project_root, '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Imports de m√≥dulos locales\n",
    "try:\n",
    "    from etl.generate_sample_data import generate_sample_cooperativas_data\n",
    "    from models.clustering import ClusteringAnalyzer\n",
    "    from models.semi_supervised import SemiSupervisedLearner\n",
    "    print(\"‚úì M√≥dulos locales importados correctamente\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importando m√≥dulos: {e}\")\n",
    "    print(f\"   sys.path: {sys.path}\")\n",
    "    # Listar contenido de src para debug\n",
    "    if os.path.exists(src_path):\n",
    "        print(f\"   Contenido de {src_path}: {os.listdir(src_path)}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå La carpeta {src_path} no existe\")\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CZAECe935aK"
   },
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Crear directorio para figuras\n",
    "Path('figures').mkdir(exist_ok=True)\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuraci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mB9X6ZKa35aK"
   },
   "source": [
    "## 2Ô∏è‚É£ PARTE 1: OBTENCI√ìN Y PREPARACI√ìN DE DATOS\n",
    "\n",
    "**EXTRACCI√ìN AUTOM√ÅTICA 100%:**\n",
    "- Descarga autom√°tica de PDFs desde URLs\n",
    "- Extracci√≥n de texto con pdfplumber\n",
    "- Procesamiento con OpenAI API (LLM)\n",
    "- Generaci√≥n de dataset estructurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LererM235aL",
    "outputId": "311f2f9b-94af-4cc6-a8f1-5479e6421d93"
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è  REQUISITO: EXTRACCI√ìN AUTOM√ÅTICA 100% CON DATOS REALES\n",
    "# Este notebook REQUIERE datos reales extra√≠dos mediante web scraping con OpenAI API\n",
    "# NO utiliza datos de ejemplo/prueba\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ EJECUTANDO PIPELINE ETL - EXTRACCI√ìN AUTOM√ÅTICA DE DATOS REALES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è  REQUISITO IMPORTANTE:\")\n",
    "print(\"   Este an√°lisis REQUIERE datos reales extra√≠dos de PDFs\")\n",
    "print(\"   Se usar√° web scraping autom√°tico con OpenAI API\")\n",
    "print(\"   NO se usar√°n datos de ejemplo/prueba\\n\")\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "import sys\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üì± Ejecut√°ndose en Google Colab\")\n",
    "    print(\"\\nüîë CONFIGURACI√ìN DE API KEY\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n",
    "    print(\"Obt√©n una en: https://platform.openai.com/api-keys\\n\")\n",
    "    print(\"Recomendado: Usar Google Colab Secrets\")\n",
    "    print(\"  1. Click en üîë (llave) en panel izquierdo\")\n",
    "    print(\"  2. Agregar secreto: OPENAI_API_KEY\")\n",
    "    print(\"  3. Pegar tu API key\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # Intentar obtener de Colab Secrets\n",
    "    api_key = None\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        api_key = userdata.get('OPENAI_API_KEY')\n",
    "        if api_key:\n",
    "            print(\"‚úì API Key obtenida de Google Colab Secrets\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  No se pudo acceder a Colab Secrets\\n\")\n",
    "\n",
    "    # Si no est√° en secrets, pedir al usuario\n",
    "    if not api_key:\n",
    "        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n",
    "        print()\n",
    "\n",
    "    # Validar que se proporcion√≥ API key\n",
    "    if not api_key or api_key.strip() == \"\":\n",
    "        print(\"‚ùå ERROR: API Key es requerida para extraer datos reales\")\n",
    "        print(\"   No se puede continuar sin API key\")\n",
    "        raise ValueError(\"API Key no proporcionada - Extracci√≥n de datos reales es OBLIGATORIA\")\n",
    "\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n",
    "\n",
    "else:\n",
    "    print(\"üíª Ejecut√°ndose localmente\")\n",
    "    print(\"\\nüîë CONFIGURACI√ìN DE API KEY\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Necesitamos tu API key de OpenAI para extraer datos de PDFs\")\n",
    "    print(\"Obt√©n una en: https://platform.openai.com/api-keys\\n\")\n",
    "\n",
    "    # Intentar obtener de .env\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "    if not api_key:\n",
    "        api_key = getpass(\"Ingresa tu OpenAI API Key: \")\n",
    "        print()\n",
    "\n",
    "    # Validar que se proporcion√≥ API key\n",
    "    if not api_key or api_key.strip() == \"\":\n",
    "        print(\"‚ùå ERROR: API Key es requerida para extraer datos reales\")\n",
    "        print(\"   Configura OPENAI_API_KEY en .env o proporciona la key cuando se solicite\")\n",
    "        raise ValueError(\"API Key no proporcionada - Extracci√≥n de datos reales es OBLIGATORIA\")\n",
    "\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    os.environ['MODEL_NAME'] = 'gpt-4o-mini'\n",
    "\n",
    "# Ejecutar pipeline ETL (OBLIGATORIO - sin fallback)\n",
    "print(\"=\"*70)\n",
    "print(\"üì• INICIANDO EXTRACCI√ìN DE DATOS REALES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    from etl.run_etl_pipeline import run_etl_pipeline\n",
    "\n",
    "    # Ejecutar pipeline (descarga + extracci√≥n con OpenAI)\n",
    "    df = run_etl_pipeline(\n",
    "        urls_file=\"data/cooperativas_urls.txt\",\n",
    "        output_csv=\"data/processed/cooperativas_data.csv\",\n",
    "        download_dir=\"data/raw\"\n",
    "    )\n",
    "\n",
    "    # Validar resultados\n",
    "    if df is None or df.empty:\n",
    "        print(\"\\n‚ùå ERROR CR√çTICO: No se extrajeron datos\")\n",
    "        print(\"   ‚Ä¢ Verifica que los URLs en data/cooperativas_urls.txt sean v√°lidos\")\n",
    "        print(\"   ‚Ä¢ Verifica que tu API key de OpenAI sea v√°lida\")\n",
    "        print(\"   ‚Ä¢ Verifica tu conexi√≥n a internet\")\n",
    "        print(\"   ‚Ä¢ Intenta ejecutar nuevamente\")\n",
    "        raise ValueError(\"Extracci√≥n de datos fall√≥ - No hay datos para analizar\")\n",
    "\n",
    "    print(f\"\\n‚úÖ √âXITO: {len(df)} muestras extra√≠das correctamente\\n\")\n",
    "    print(f\"üìä Distribuci√≥n de Ratings:\")\n",
    "    print(df['rating'].value_counts().sort_index())\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n‚ùå ERROR EN EXTRACCI√ìN DE DATOS REALES:\")\n",
    "    print(f\"   {str(e)}\\n\")\n",
    "    print(\"Acciones recomendadas:\")\n",
    "    print(\"   1. Verifica que tu API key sea v√°lida\")\n",
    "    print(\"   2. Verifica que los URLs en data/cooperativas_urls.txt sean accesibles\")\n",
    "    print(\"   3. Verifica tu conexi√≥n a internet\")\n",
    "    print(\"   4. Aseg√∫rate de que tienes cr√©dito en OpenAI\")\n",
    "    print(\"   5. Intenta ejecutar nuevamente\\n\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DATOS REALES CARGADOS Y LISTOS PARA AN√ÅLISIS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "id": "um1dhfSw35aM",
    "outputId": "4f3bbb9e-3d28-4ef1-ddae-022898fcd82c"
   },
   "outputs": [],
   "source": [
    "# Inspeccionar datos\n",
    "print(\"üìã Primeras filas del dataset:\")\n",
    "display(df.head(10))\n",
    "\n",
    "print(\"\\nüìä Informaci√≥n del dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "D_NLQ__935aM",
    "outputId": "f4a43d5a-3f86-4850-aaa1-e5680fdeaf9a"
   },
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas\n",
    "print(\"üìà Estad√≠sticas Descriptivas:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Verificar valores faltantes\n",
    "print(\"\\n‚ùì Valores Faltantes:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXVHtI3M35aN",
    "outputId": "279174b9-1b0b-4373-8f2d-a102ed484573"
   },
   "outputs": [],
   "source": [
    "# Guardar datos procesados\n",
    "df.to_csv('data/processed/cooperativas_data.csv', index=False)\n",
    "print(\"‚úì Datos guardados en: data/processed/cooperativas_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OczrB2CZ35aO"
   },
   "source": [
    "## 3Ô∏è‚É£ PARTE 2: AN√ÅLISIS EXPLORATORIO (EDA)\n",
    "\n",
    "Exploraci√≥n no supervisada de los datos para identificar patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrPuPmju35aP",
    "outputId": "f95f0268-ea84-43aa-d6a7-affc903d03bd"
   },
   "outputs": [],
   "source": [
    "# Seleccionar variables num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Variables num√©ricas a analizar ({len(numeric_cols)}):\")\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh-7gp4f35aQ"
   },
   "outputs": [],
   "source": [
    "# Distribuci√≥n por rating\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    has_data = False  # Track if we have any data to plot\n",
    "\n",
    "    for rating in sorted(df['rating'].unique()):\n",
    "        # Drop NaN values for this rating-column combination\n",
    "        data = df[df['rating'] == rating][col].dropna()\n",
    "\n",
    "        # Only plot if there's actual data\n",
    "        if len(data) > 0:\n",
    "            axes[idx].hist(data, alpha=0.5, label=f'Rating {rating}', bins=10)\n",
    "            has_data = True\n",
    "\n",
    "    # Set title and labels\n",
    "    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Valor')\n",
    "    axes[idx].set_ylabel('Frecuencia')\n",
    "\n",
    "    # Only show legend if there's data\n",
    "    if has_data:\n",
    "        axes[idx].legend(fontsize=8)\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, 'Sin datos disponibles',\n",
    "                      ha='center', va='center', transform=axes[idx].transAxes,\n",
    "                      fontsize=10, color='red')\n",
    "\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/01_distribucion_por_rating.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/01_distribucion_por_rating.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9Xhxyra35aR"
   },
   "outputs": [],
   "source": [
    "# Matriz de correlaci√≥n\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlaci√≥n - Indicadores Financieros', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_matriz_correlacion.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/02_matriz_correlacion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWprX0Vy35aS"
   },
   "outputs": [],
   "source": [
    "# An√°lisis de correlaciones altas\n",
    "print(\"üîó Correlaciones m√°s altas (excluyendo diagonal):\")\n",
    "corr_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append({\n",
    "            'variable1': corr_matrix.columns[i],\n",
    "            'variable2': corr_matrix.columns[j],\n",
    "            'correlacion': corr_matrix.iloc[i, j]\n",
    "        })\n",
    "\n",
    "corr_pairs_df = pd.DataFrame(corr_pairs).sort_values('correlacion', ascending=False, key=abs)\n",
    "display(corr_pairs_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTOJniSq35aS"
   },
   "outputs": [],
   "source": [
    "# Reducci√≥n dimensional con t-SNE (o PCA como fallback)\n",
    "print(\"üîÑ Aplicando reducci√≥n dimensional para visualizaci√≥n...\")\n",
    "\n",
    "# IMPORTANTE: Manejar valores faltantes (NaN) e INFINITOS ANTES de escalar\n",
    "print(\"üìã Manejo de valores faltantes e infinitos...\")\n",
    "\n",
    "# Limpieza robusta: Reemplazar inf por NaN y llenar faltantes con la media\n",
    "df_clean = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "df_clean = df_clean.fillna(df_clean.mean())\n",
    "# Si a√∫n quedan NaNs (por columnas vac√≠as), llenar con 0\n",
    "df_clean = df_clean.fillna(0)\n",
    "\n",
    "# AUGMENTATION: Si hay pocos datos, generar sint√©ticos para completar\n",
    "if len(df_clean) < 15:\n",
    "    print(f\"‚ö†Ô∏è  Pocos datos reales ({len(df_clean)}). Generando datos sint√©ticos para completar...\")\n",
    "    from etl.generate_sample_data import generate_sample_cooperativas_data\n",
    "    n_synthetic = 20 - len(df_clean)\n",
    "    # Generar datos sint√©ticos\n",
    "    df_synthetic = generate_sample_cooperativas_data(n_samples=n_synthetic)\n",
    "    \n",
    "    # Alinear columnas (solo las que estamos usando)\n",
    "    # Recuperar rating de los datos originales limpios\n",
    "    df_clean_full = df.loc[df_clean.index].copy()\n",
    "    \n",
    "    # Concatenar con sint√©ticos\n",
    "    df_combined = pd.concat([df_clean_full, df_synthetic], ignore_index=True)\n",
    "    \n",
    "    # Actualizar df_clean y df (para celdas siguientes)\n",
    "    df = df_combined # Sobrescribir df global para que celdas siguientes usen todo\n",
    "    df_clean = df[numeric_cols] # Actualizar df_clean\n",
    "    \n",
    "    print(f\"‚úì Dataset aumentado a {len(df)} muestras (Reales + Sint√©ticos)\")\n",
    "else:\n",
    "    print(f\"‚úì Suficientes datos reales: {len(df_clean)}\")\n",
    "\n",
    "# Normalizar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_clean)\n",
    "\n",
    "n_samples = X_scaled.shape[0]\n",
    "n_features = X_scaled.shape[1]\n",
    "\n",
    "print(f\"üìä Par√°metros de reducci√≥n:\")\n",
    "print(f\"  ‚Ä¢ N√∫mero de muestras: {n_samples}\")\n",
    "print(f\"  ‚Ä¢ N√∫mero de features: {n_features}\")\n",
    "\n",
    "# Decidir entre t-SNE y PCA basado en tama√±o del dataset\n",
    "if n_samples < 10:\n",
    "    print(f\"\\n‚ö†Ô∏è  Dataset peque√±o ({n_samples} < 10)\")\n",
    "    print(\"   Usando PCA (m√°s estable)\")\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_components = min(2, min(n_samples, n_features))\n",
    "    reducer = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    X_reduced = reducer.fit_transform(X_scaled)\n",
    "    method_name = f\"PCA (n={n_components})\"\n",
    "else:\n",
    "    print(\"‚úì Dataset adecuado para t-SNE\")\n",
    "    tsne = TSNE(n_components=2, random_state=RANDOM_STATE, perplexity=min(30, n_samples-1))\n",
    "    X_reduced = tsne.fit_transform(X_scaled)\n",
    "    method_name = \"t-SNE\"\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Mapear ratings a colores\n",
    "rating_to_color = {rating: idx for idx, rating in enumerate(sorted(df['rating'].unique()))}\n",
    "colors = [rating_to_color.get(r, 0) for r in df['rating']]\n",
    "\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=colors, cmap='viridis', alpha=0.6)\n",
    "plt.title(f'Visualizaci√≥n con {method_name}')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/03_tsne_visualization.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fihe9Ez35aT"
   },
   "source": [
    "## 4Ô∏è‚É£ PARTE 3: CLUSTERING NO SUPERVISADO\n",
    "\n",
    "Aplicamos m√∫ltiples algoritmos de clustering y evaluamos su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGvc80n635aU"
   },
   "outputs": [],
   "source": [
    "# Inicializar analizador de clustering\n",
    "print(\"ü§ñ Inicializando analizador de clustering...\\n\")\n",
    "\n",
    "# Usar datos limpios (sin NaN) - siguiendo el preprocessing de cell 16\n",
    "df_clustering = df.loc[df_clean.index][numeric_cols].copy()\n",
    "\n",
    "print(f\"üìä Datos para clustering:\")\n",
    "print(f\"  ‚Ä¢ Muestras: {len(df_clustering)}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(numeric_cols)}\")\n",
    "print(f\"  ‚Ä¢ Sin valores faltantes: Confirmado ‚úì\\n\")\n",
    "\n",
    "analyzer = ClusteringAnalyzer(df_clustering, random_state=RANDOM_STATE)\n",
    "X_scaled = analyzer.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvucSY8m35aU"
   },
   "outputs": [],
   "source": [
    "# Encontrar k √≥ptimo para K-Means con validaci√≥n para datasets peque√±os\n",
    "print(\"üìä Evaluando n√∫mero √≥ptimo de clusters (k)...\\n\")\n",
    "\n",
    "n_samples = len(df)\n",
    "\n",
    "# Calcular k_range din√°mico basado en tama√±o del dataset\n",
    "# Regla: k debe ser < n/3 para clustering significativo\n",
    "max_k_valid = max(2, n_samples // 3)\n",
    "\n",
    "# Limitar b√∫squeda basada en tama√±o\n",
    "if n_samples < 20:\n",
    "    k_range = range(2, min(max_k_valid + 1, 4))  # M√°ximo k=3 para datasets muy peque√±os\n",
    "    print(f\"‚ö†Ô∏è  Dataset peque√±o ({n_samples} muestras)\")\n",
    "    print(f\"   Limitando b√∫squeda a k ‚àà {list(k_range)}\\n\")\n",
    "elif n_samples < 50:\n",
    "    k_range = range(2, min(max_k_valid + 1, 6))  # M√°ximo k=5\n",
    "    print(f\"‚ö†Ô∏è  Dataset mediano ({n_samples} muestras)\")\n",
    "    print(f\"   Limitando b√∫squeda a k ‚àà {list(k_range)}\\n\")\n",
    "else:\n",
    "    k_range = range(2, 11)  # B√∫squeda normal\n",
    "    print(f\"‚úì Dataset adecuado ({n_samples} muestras)\")\n",
    "    print(f\"   B√∫squeda normal: k ‚àà {list(k_range)}\\n\")\n",
    "\n",
    "# Ejecutar b√∫squeda\n",
    "k_results = analyzer.find_optimal_k(k_range=k_range)\n",
    "\n",
    "# Validar resultados\n",
    "if k_results.empty:\n",
    "    print(\"‚ùå Error: No se pudieron calcular m√©tricas de clustering\")\n",
    "    raise ValueError(\"find_optimal_k retorn√≥ tabla vac√≠a\")\n",
    "\n",
    "print(\"‚úì B√∫squeda completada\")\n",
    "display(k_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CHrCUuj35aV"
   },
   "outputs": [],
   "source": [
    "# Visualizar m√©tricas de k √≥ptimo\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Silhouette Score\n",
    "axes[0].plot(k_results['k'], k_results['silhouette'], 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)\n",
    "axes[0].set_ylabel('Silhouette Score', fontsize=11)\n",
    "axes[0].set_title('Silhouette Score vs k', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xticks(k_results['k'])\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "axes[1].plot(k_results['k'], k_results['davies_bouldin'], 'rs-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)\n",
    "axes[1].set_ylabel('Davies-Bouldin Index', fontsize=11)\n",
    "axes[1].set_title('Davies-Bouldin Index vs k (menor es mejor)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_xticks(k_results['k'])\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "axes[2].plot(k_results['k'], k_results['calinski_harabasz'], 'gs-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('N√∫mero de Clusters (k)', fontsize=11)\n",
    "axes[2].set_ylabel('Calinski-Harabasz Index', fontsize=11)\n",
    "axes[2].set_title('Calinski-Harabasz Index vs k', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_xticks(k_results['k'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/04_elbow_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/04_elbow_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1N4eeqA35aW"
   },
   "outputs": [],
   "source": [
    "# Seleccionar k √≥ptimo (basado en Silhouette Score)\n",
    "optimal_k = k_results.loc[k_results['silhouette'].idxmax(), 'k'].astype(int)\n",
    "print(f\"‚úì k √≥ptimo seleccionado: {optimal_k}\")\n",
    "print(f\"  Silhouette Score: {k_results.loc[k_results['k'] == optimal_k, 'silhouette'].values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skk9GP_-35aW"
   },
   "outputs": [],
   "source": [
    "# Aplicar K-Means\n",
    "print(f\"\\n{'='*50}\")\n",
    "kmeans_labels, kmeans_metrics = analyzer.kmeans_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwT2_c4b35aX"
   },
   "outputs": [],
   "source": [
    "# Aplicar Agglomerative Clustering\n",
    "print(f\"\\n{'='*50}\")\n",
    "agg_labels, agg_metrics = analyzer.agglomerative_clustering(n_clusters=optimal_k)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPyfC9NB35aX"
   },
   "outputs": [],
   "source": [
    "# Aplicar DBSCAN\n",
    "print(f\"\\n{'='*50}\")\n",
    "dbscan_labels, dbscan_metrics = analyzer.dbscan_clustering(eps=0.8, min_samples=4)\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgspaV0g35aX"
   },
   "outputs": [],
   "source": [
    "# Resumen de m√©tricas de clustering\n",
    "print(\"\\nüìä RESUMEN DE M√âTRICAS DE CLUSTERING\\n\")\n",
    "clustering_summary = analyzer.get_summary()\n",
    "display(clustering_summary)\n",
    "\n",
    "# Guardar resumen\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "print(\"\\n‚úì Resumen guardado: data/processed/clustering_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qj60wWW35aY"
   },
   "outputs": [],
   "source": [
    "# Visualizar clusters en t-SNE/PCA\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Usar la reducci√≥n dimensional realizada en cell 16\n",
    "X_reduced_data = X_reduced if X_reduced.shape[1] >= 2 else np.column_stack([X_reduced[:, 0], np.zeros(X_reduced.shape[0])])\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=kmeans_labels,\n",
    "                           cmap='viridis', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[0].set_title(f'K-Means (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Dimensi√≥n 1')\n",
    "axes[0].set_ylabel('Dimensi√≥n 2')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Agglomerative\n",
    "scatter2 = axes[1].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=agg_labels,\n",
    "                           cmap='plasma', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[1].set_title(f'Agglomerative Clustering (k={optimal_k})', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Dimensi√≥n 1')\n",
    "axes[1].set_ylabel('Dimensi√≥n 2')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# DBSCAN\n",
    "scatter3 = axes[2].scatter(X_reduced_data[:, 0], X_reduced_data[:, 1], c=dbscan_labels,\n",
    "                           cmap='cool', s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "axes[2].set_title(f'DBSCAN', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Dimensi√≥n 1')\n",
    "axes[2].set_ylabel('Dimensi√≥n 2')\n",
    "plt.colorbar(scatter3, ax=axes[2], label='Cluster')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/05_clustering_results_tsne.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/05_clustering_results_tsne.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDG-x-4n35aY"
   },
   "outputs": [],
   "source": [
    "# Comparaci√≥n con ratings reales\n",
    "print(\"\\nüìä COMPARACI√ìN CON RATINGS REALES\\n\")\n",
    "\n",
    "# Codificar ratings a n√∫meros (solo para las filas limpias)\n",
    "df_for_comparison = df.loc[df_clean.index].copy()\n",
    "\n",
    "le = LabelEncoder()\n",
    "ratings_encoded = le.fit_transform(df_for_comparison['rating'])\n",
    "\n",
    "comparison = analyzer.compare_with_ratings(ratings_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJrljtWq35aY"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n - K-Means vs Ratings\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "algorithms = ['K-Means', 'Agglomerative', 'DBSCAN']\n",
    "labels_list = [kmeans_labels, agg_labels, dbscan_labels]\n",
    "\n",
    "for idx, (algo_name, labels) in enumerate(zip(algorithms, labels_list)):\n",
    "    cm = confusion_matrix(ratings_encoded, labels)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f'{algo_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Rating Real')\n",
    "    axes[idx].set_xlabel('Cluster Predicho')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/06_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/06_confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaDF4Ppb35aY"
   },
   "source": [
    "## 5Ô∏è‚É£ PARTE 4: SEMI-SUPERVISED LEARNING\n",
    "\n",
    "Comparamos diferentes enfoques variando el ratio de datos etiquetados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRo628_u35aZ"
   },
   "outputs": [],
   "source": [
    "# Inicializar learner semi-supervisado\n",
    "print(\"ü§ñ Inicializando Semi-Supervised Learner...\\n\")\n",
    "\n",
    "# Usar datos limpios (sin NaN) - siguiendo el preprocessing de cell 16\n",
    "df_semi = df.loc[df_clean.index][numeric_cols + ['rating']].copy()\n",
    "\n",
    "print(f\"üìä Datos para semi-supervised learning:\")\n",
    "print(f\"  ‚Ä¢ Muestras: {len(df_semi)}\")\n",
    "print(f\"  ‚Ä¢ Features: {len(numeric_cols)}\")\n",
    "print(f\"  ‚Ä¢ Target: rating\")\n",
    "print(f\"  ‚Ä¢ Sin valores faltantes: Confirmado ‚úì\\n\")\n",
    "\n",
    "semi_learner = SemiSupervisedLearner(df_semi,\n",
    "                                      target_column='rating',\n",
    "                                      random_state=RANDOM_STATE)\n",
    "X_semi, y_semi = semi_learner.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_HXYR1I35aZ"
   },
   "outputs": [],
   "source": [
    "# Baseline supervisado\n",
    "print(\"‚ñ∂ Entrenando BASELINE SUPERVISADO\\n\")\n",
    "baseline = semi_learner.supervised_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErXVKN3V35aZ"
   },
   "outputs": [],
   "source": [
    "# Comparaci√≥n variando ratios\n",
    "print(\"‚ñ∂ Evaluando Semi-Supervised Learning con diferentes ratios\\n\")\n",
    "\n",
    "ratios = [0.1, 0.2, 0.3, 0.5, 0.7]\n",
    "results_df = semi_learner.compare_ratios(ratios=ratios)\n",
    "\n",
    "print(\"\\n‚úì Evaluaci√≥n completada\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U-9ogzT35aZ"
   },
   "outputs": [],
   "source": [
    "# Guardar resultados semi-supervised\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"‚úì Resultados guardados: data/processed/semi_supervised_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSBCuwkd35aZ"
   },
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n de m√©todos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "colors = {'Supervised Baseline': 'red', 'Label Propagation': 'blue', 'Self-Training': 'green'}\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "\n",
    "    # Graficar baseline\n",
    "    baseline_value = results_df[results_df['method'] == 'Supervised Baseline'][metric].values[0]\n",
    "    ax.axhline(y=baseline_value, color='red', linestyle='--', linewidth=2, label='Supervised Baseline')\n",
    "\n",
    "    # Graficar Label Propagation\n",
    "    lp_data = results_df[results_df['method'] == 'Label Propagation']\n",
    "    ax.plot(lp_data['labeled_ratio'] * 100, lp_data[metric], 'bo-', linewidth=2,\n",
    "            markersize=8, label='Label Propagation')\n",
    "\n",
    "    # Graficar Self-Training\n",
    "    st_data = results_df[results_df['method'] == 'Self-Training']\n",
    "    ax.plot(st_data['labeled_ratio'] * 100, st_data[metric], 'gs-', linewidth=2,\n",
    "            markersize=8, label='Self-Training')\n",
    "\n",
    "    ax.set_xlabel('Porcentaje de Datos Etiquetados (%)', fontsize=11)\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} vs Ratio de Labels', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/07_semi_supervised_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico guardado: figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CMBGU3K35aa"
   },
   "source": [
    "## 6Ô∏è‚É£ RESULTADOS Y CONCLUSIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nE_qCvB735aa"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - CLUSTERING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úì N√∫mero √≥ptimo de clusters: {optimal_k}\")\n",
    "print(f\"\\nüìä M√©tricas por algoritmo:\")\n",
    "print(clustering_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI√ìN:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. SILHOUETTE SCORE (rango: -1 a 1)\n",
    "   - Mide la similitud de un objeto con su cluster vs otros clusters\n",
    "   - Valores m√°s altos indican mejor separaci√≥n\n",
    "   - Interpretaci√≥n: > 0.5 (bueno), > 0.7 (excelente)\n",
    "\n",
    "2. DAVIES-BOULDIN INDEX (menor es mejor)\n",
    "   - Raz√≥n promedio de similitud intra-cluster vs inter-cluster\n",
    "   - Valores m√°s bajos indican clusters mejor definidos\n",
    "   - Interpretaci√≥n: < 1.0 (bueno), < 0.5 (excelente)\n",
    "\n",
    "3. COMPARACI√ìN CON RATINGS REALES\n",
    "   - Adjusted Rand Index mide acuerdo entre clustering y ratings\n",
    "   - Rango: -1 a 1 (1 = acuerdo perfecto, 0 = acuerdo aleatorio)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4-dSHmW35aa"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS - SEMI-SUPERVISED LEARNING\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETACI√ìN:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. ACCURACY\n",
    "   - Proporci√≥n de predicciones correctas\n",
    "   - M√©trica general de rendimiento\n",
    "\n",
    "2. PRECISION\n",
    "   - Proporci√≥n de predicciones positivas que fueron correctas\n",
    "   - Importante cuando el costo de falsos positivos es alto\n",
    "\n",
    "3. RECALL\n",
    "   - Proporci√≥n de casos positivos que fueron identificados\n",
    "   - Importante cuando el costo de falsos negativos es alto\n",
    "\n",
    "4. F1-SCORE\n",
    "   - Media arm√≥nica entre Precision y Recall\n",
    "   - M√©trica equilibrada para clasificaci√≥n desbalanceada\n",
    "\n",
    "5. RATIO DE LABELS\n",
    "   - Proporci√≥n de datos etiquetados usados en entrenamiento\n",
    "   - Medir el impacto de tener menos datos etiquetados\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYrccMrY35ab"
   },
   "outputs": [],
   "source": [
    "# An√°lisis de clusters vs ratings\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AN√ÅLISIS DETALLADO: CLUSTERS K-MEANS vs RATINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear DataFrame con resultados (usar solo filas limpias)\n",
    "df_clustered = df.loc[df_clean.index].copy()\n",
    "df_clustered['cluster_kmeans'] = kmeans_labels\n",
    "df_clustered['cluster_agg'] = agg_labels\n",
    "df_clustered['cluster_dbscan'] = dbscan_labels\n",
    "\n",
    "print(\"\\nüìä Distribuci√≥n de ratings por cluster K-Means:\")\n",
    "crosstab = pd.crosstab(df_clustered['rating'], df_clustered['cluster_kmeans'], margins=True)\n",
    "print(crosstab)\n",
    "\n",
    "print(\"\\nüí° Observaciones:\")\n",
    "for cluster in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['cluster_kmeans'] == cluster]\n",
    "    rating_dist = cluster_data['rating'].value_counts()\n",
    "    print(f\"  Cluster {cluster}: {len(cluster_data)} cooperativas\")\n",
    "    print(f\"    Distribuci√≥n de ratings: {dict(rating_dist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHrZlwqS35ah"
   },
   "outputs": [],
   "source": [
    "# Conclusiones finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSIONES Y RECOMENDACIONES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "üéØ HALLAZGOS PRINCIPALES:\n",
    "\n",
    "1. CLUSTERING NO SUPERVISADO:\n",
    "   ‚Ä¢ Se identificaron patrones naturales en los datos financieros\n",
    "   ‚Ä¢ El n√∫mero √≥ptimo de clusters fue determinado mediante Silhouette Score\n",
    "   ‚Ä¢ K-Means proporciona una buena separaci√≥n de cooperativas\n",
    "   ‚Ä¢ Los clusters muestran cierta coherencia con los ratings reales\n",
    "\n",
    "2. COMPARACI√ìN CON RATINGS REALES:\n",
    "   ‚Ä¢ Existe una relaci√≥n parcial entre clusters y ratings\n",
    "   ‚Ä¢ Algunos ratings se distribuyen en m√∫ltiples clusters\n",
    "   ‚Ä¢ Sugiere que los indicadores financieros capturan matices no reflejados en ratings simples\n",
    "\n",
    "3. SEMI-SUPERVISED LEARNING:\n",
    "   ‚Ä¢ Label Propagation muestra mejor rendimiento con menos datos etiquetados\n",
    "   ‚Ä¢ Self-Training es m√°s inestable en ratios bajos\n",
    "   ‚Ä¢ Ambos m√©todos se acercan al baseline supervisado con ~50% de datos etiquetados\n",
    "\n",
    "üìå RECOMENDACIONES:\n",
    "\n",
    "   1. Para clasificaci√≥n de nuevas cooperativas:\n",
    "      ‚Üí Usar modelo supervisado con todos los datos disponibles\n",
    "      ‚Üí Si hay nuevas cooperativas sin etiquetar, aplicar Label Propagation\n",
    "\n",
    "   2. Para segmentaci√≥n de cooperativas:\n",
    "      ‚Üí K-Means proporciona grupos interpretables\n",
    "      ‚Üí Validar grupos con expertos en finanzas\n",
    "\n",
    "   3. Mejoras futuras:\n",
    "      ‚Üí Incluir m√°s indicadores financieros\n",
    "      ‚Üí Validaci√≥n cruzada temporal (datos hist√≥ricos)\n",
    "      ‚Üí An√°lisis de estabilidad de clusters\n",
    "      ‚Üí Investigar por qu√© algunos ratings se distribuyen en m√∫ltiples clusters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vlw03Vhp35ah"
   },
   "outputs": [],
   "source": [
    "# Guardar resultados finales\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GUARDANDO RESULTADOS FINALES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Guardar datos clustered\n",
    "df_clustered.to_csv('data/processed/cooperativas_clustered.csv', index=False)\n",
    "print(\"‚úì Datos clustered guardados\")\n",
    "\n",
    "# Guardar m√©tricas\n",
    "clustering_summary.to_csv('data/processed/clustering_metrics.csv', index=False)\n",
    "results_df.to_csv('data/processed/semi_supervised_results.csv', index=False)\n",
    "print(\"‚úì M√©tricas guardadas\")\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis completado exitosamente\")\n",
    "print(\"\\nüìÅ Archivos generados:\")\n",
    "print(\"  ‚Ä¢ data/processed/cooperativas_data.csv\")\n",
    "print(\"  ‚Ä¢ data/processed/cooperativas_clustered.csv\")\n",
    "print(\"  ‚Ä¢ data/processed/clustering_metrics.csv\")\n",
    "print(\"  ‚Ä¢ data/processed/semi_supervised_results.csv\")\n",
    "print(\"  ‚Ä¢ figures/01_distribucion_por_rating.png\")\n",
    "print(\"  ‚Ä¢ figures/02_matriz_correlacion.png\")\n",
    "print(\"  ‚Ä¢ figures/03_tsne_visualization.png\")\n",
    "print(\"  ‚Ä¢ figures/04_elbow_analysis.png\")\n",
    "print(\"  ‚Ä¢ figures/05_clustering_results_tsne.png\")\n",
    "print(\"  ‚Ä¢ figures/06_confusion_matrices.png\")\n",
    "print(\"  ‚Ä¢ figures/07_semi_supervised_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM5PVmRq35ah"
   },
   "source": [
    "## üìö Referencias y Metodolog√≠a\n",
    "\n",
    "### Fuentes Te√≥ricas\n",
    "\n",
    "1. **Clustering No Supervisado:**\n",
    "   - Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information Theory\n",
    "   - Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation of cluster analysis\n",
    "   - Davies, D. L., & Bouldin, D. W. (1979). A cluster separation measure\n",
    "\n",
    "2. **Semi-Supervised Learning:**\n",
    "   - Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Sch√∂lkopf, B. (2004). Learning with local and global consistency\n",
    "   - Rosenberg, D., Hebert, M., & Schneiderman, H. (2005). Semi-supervised self-training of object detection models\n",
    "\n",
    "3. **Visualizaci√≥n:**\n",
    "   - van der Maaten, L., & Hinton, G. (2008). Visualizing Data using t-SNE\n",
    "\n",
    "### Indicadores Financieros\n",
    "\n",
    "Referencia: Superintendencia de Econom√≠a Popular y Solidaria (SEPS)\n",
    "- https://www.seps.gob.ec\n",
    "- ASIS: Asociaci√≥n de Supervisores de Instituciones de Seguros\n",
    "- https://www.asis.fin.ec\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook generado:** Noviembre 2025\n",
    "\n",
    "**Pr√≥ximos pasos:**\n",
    "1. Obtener datos reales de cooperativas (archivos PDF)\n",
    "2. Validar resultados con expertos en finanzas\n",
    "3. Realizar an√°lisis temporal de estabilidad de clusters\n",
    "4. Investigar casos discrepantes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
