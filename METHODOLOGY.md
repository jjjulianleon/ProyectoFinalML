# üî¨ Metodolog√≠a del Proyecto

## üìã Resumen Ejecutivo

Este proyecto implementa t√©cnicas de **Machine Learning no supervisado y semi-supervisado** para analizar y agrupar cooperativas de ahorro y cr√©dito del Segmento 1 en Ecuador seg√∫n sus indicadores financieros.

---

## üéØ Objetivos

### Objetivos Principales

1. **Identificar grupos naturales** de cooperativas con comportamientos financieros similares
2. **Validar coherencia** entre clusters autom√°ticos y ratings de riesgo externos
3. **Comparar enfoques** supervisados vs semi-supervisados
4. **Evaluar impacto** de cantidad de datos etiquetados en rendimiento

### Hip√≥tesis

- Los indicadores financieros contienen patrones que permiten agrupar cooperativas naturalmente
- Estos grupos mostrar√°n cierta correlaci√≥n con ratings de riesgo asignados externamente
- El aprendizaje semi-supervisado puede aproximar rendimiento supervisado con menos datos etiquetados

---

## üìä FASE 1: ADQUISICI√ìN Y PREPARACI√ìN DE DATOS

### 1.1 Obtenci√≥n de Datos

**M√©todo:** Web Scraping + Extracci√≥n con LLM

```
PDFs de Reportes Financieros
            ‚Üì
    [PDF Downloader]
            ‚Üì
Archivos PDF en data/raw/
            ‚Üì
    [PDF Text Reader]
            ‚Üì
Texto extra√≠do
            ‚Üì
    [OpenAI API]
            ‚Üì
JSON Estructurado
            ‚Üì
    [Consolidaci√≥n]
            ‚Üì
CSV consolidado ‚Üí data/processed/cooperativas_data.csv
```

**Fuentes:**
- SEPS (Superintendencia de Econom√≠a Popular y Solidaria)
- ASIS (Reportes de indicadores financieros)
- Reportes directos de cooperativas

**Variables extra√≠das:** 12+ indicadores financieros

### 1.2 Limpieza de Datos

```python
# Validaciones realizadas:
1. Verificar valores faltantes
2. Detectar outliers estad√≠sticos
3. Validar rangos de valores (0-1 para ratios)
4. Remover duplicados
5. Verificar tipos de datos
```

**Resultado esperado:**
- 50-200 cooperativas
- 12 variables num√©ricas + 2 categ√≥ricas
- 0% valores faltantes (despu√©s de limpieza)

---

## üìà FASE 2: AN√ÅLISIS EXPLORATORIO (EDA)

### 2.1 Estad√≠stica Descriptiva

```python
# Para cada variable:
- Media, mediana, desviaci√≥n est√°ndar
- Rango (m√≠n, m√°x)
- Percentiles (Q1, Q3)
- Distribuci√≥n por rating
```

### 2.2 An√°lisis de Correlaciones

```python
# Crear matriz de correlaci√≥n de Pearson
# Objetivo: Identificar:
- Variables altamente correlacionadas (|r| > 0.8)
- Posibles redundancias
- Relaciones lineales con el rating
```

### 2.3 Reducci√≥n Dimensional (t-SNE)

```python
# Aplicar t-SNE para visualizaci√≥n 2D
# Par√°metros:
- n_components = 2
- perplexity = 30
- n_iter = 1000
- random_state = 42

# Objetivo: Visualizar separaci√≥n natural de cooperativas
# Color: Rating real
# Resultado: Gr√°fico interactivo
```

### 2.4 Visualizaciones Generadas

1. **Distribuciones por rating** - Histogramas de cada variable
2. **Matriz de correlaci√≥n** - Heatmap de correlaciones
3. **t-SNE plot** - Espacio 2D coloreado por rating
4. **Box plots** - Distribuci√≥n por grupo

---

## ü§ñ FASE 3: CLUSTERING NO SUPERVISADO

### 3.1 Normalizaci√≥n de Datos

```python
# Aplicar StandardScaler
# Raz√≥n: Diferentes escalas entre variables
X_scaled = StandardScaler().fit_transform(X)
# Resultado: Media=0, Desv.Est.=1 para cada variable
```

### 3.2 Algoritmo 1: K-Means (Baseline)

**Caracter√≠stica:** Partition-based clustering

```
1. Inicializar k centroides aleatoriamente
2. Asignar cada punto al centroide m√°s cercano
3. Recalcular centroides como media de los clusters
4. Repetir hasta convergencia
```

**Ventajas:**
- R√°pido y escalable
- F√°cil de interpretar
- Determin√≠stico (con seed)

**Desventajas:**
- Requiere especificar k de antemano
- Sensible a inicializaci√≥n
- Asume clusters esf√©ricos

**Hiperpar√°metros:**
- `n_clusters`: Determinado por Elbow Method / Silhouette
- `n_init`: 10 (n√∫mero de veces a correr)
- `random_state`: 42 (reproducibilidad)

### 3.3 Algoritmo 2: Agglomerative Clustering

**Caracter√≠stica:** Hierarchical (bottom-up)

```
1. Inicio: cada punto es su propio cluster
2. Fusi√≥n iterativa de clusters m√°s similares
3. Hasta obtener k clusters
```

**Ventajas:**
- Produce dendrogramas (hist√≥rico de fusiones)
- No requiere inicializaci√≥n aleatoria
- M√°s estable que K-Means

**Desventajas:**
- Mayor complejidad computacional O(n¬≤)
- No es aplicable a datasets muy grandes

**Hiperpar√°metros:**
- `n_clusters`: Mismo que K-Means
- `linkage`: 'ward' (minimiza varianza intra-cluster)

### 3.4 Algoritmo 3: DBSCAN

**Caracter√≠stica:** Density-based clustering

```
1. Para cada punto no visitado:
   - Encontrar vecinos dentro de eps
   - Si >= min_samples: crear cluster
   - Expandir cluster recursivamente
2. Puntos aislados marcados como ruido (-1)
```

**Ventajas:**
- Detecta clusters de forma arbitraria
- Identifica outliers (ruido)
- No requiere especificar k

**Desventajas:**
- Sensible a hiperpar√°metros (eps, min_samples)
- Rendimiento variable en clusters de densidades diferentes

**Hiperpar√°metros:**
- `eps`: Radio m√°ximo (determinar mediante k-distance graph)
- `min_samples`: M√≠nimo puntos para formar cluster (default: 2*dim)

### 3.5 Determinaci√≥n del N√∫mero √ìptimo de Clusters

**M√©todo 1: Elbow Method**
```python
for k in range(2, 11):
    inertia = KMeans(k).fit(X).inertia_
# Gr√°ficar inertia vs k
# Codo indica k √≥ptimo
```

**M√©todo 2: Silhouette Score**
```python
# s(i) = (b(i) - a(i)) / max(a(i), b(i))
# a(i): distancia promedio a otros puntos del mismo cluster
# b(i): distancia promedio a puntos del cluster m√°s cercano
# Rango: -1 a 1 (mayor es mejor)

for k in range(2, 11):
    score = silhouette_score(X, KMeans(k).fit_predict(X))
# k con score m√°ximo es √≥ptimo
```

**M√©todo 3: Davies-Bouldin Index**
```python
# DB = 1/k * Œ£ max(Si + Sj / dij)
# Si: dispersi√≥n promedio dentro del cluster i
# dij: distancia entre centroides i y j
# Rango: 0 a ‚àû (menor es mejor)
```

---

## üìä FASE 4: EVALUACI√ìN DE CLUSTERING

### 4.1 M√©tricas Intr√≠nsecas (sin labels)

| M√©trica | F√≥rmula | Interpretaci√≥n |
|---------|---------|----------------|
| **Silhouette Score** | promedio de s(i) para todos los puntos | -1 a 1, mayor es mejor |
| **Davies-Bouldin Index** | promedio de ratios de similitud | 0 a ‚àû, menor es mejor |
| **Calinski-Harabasz Index** | ratio de dispersi√≥n entre/intra cluster | mayor es mejor |

### 4.2 M√©tricas Extr√≠nsecas (con labels reales)

| M√©trica | Definici√≥n |
|---------|-----------|
| **Adjusted Rand Index (ARI)** | Acuerdo entre dos clusterings (ajustado por azar) |
| **Normalized Mutual Information** | Informaci√≥n compartida entre clustering y labels |
| **Purity** | Proporci√≥n de puntos en cluster puro |

```python
from sklearn.metrics import adjusted_rand_score
ari = adjusted_rand_score(labels_true, labels_pred)
# -1 ‚â§ ARI ‚â§ 1
# 1: acuerdo perfecto
# 0: acuerdo aleatorio
# <0: peor que acuerdo aleatorio
```

### 4.3 Matriz de Confusi√≥n

```python
# Comparar cluster predicho vs rating real
cm = confusion_matrix(ratings_encoded, cluster_labels)
# Visualizar como heatmap
```

---

## üß† FASE 5: SEMI-SUPERVISED LEARNING

### 5.1 Enfoque 1: Baseline Supervisado

```python
# Entrenar con 100% de datos etiquetados
LogisticRegression().fit(X, y)
# Baseline de referencia para comparar
```

**M√©tricas calculadas:**
- Accuracy
- Precision (weighted)
- Recall (weighted)
- F1-Score (weighted)

### 5.2 Enfoque 2: Label Propagation

**Algoritmo:**

```
1. Crear grafo de similitud entre puntos
2. Inicializar labels conocidos
3. Propagar labels iterativamente:
   label(i) = argmax Œ£_j(kernel(i,j) * label(j))
4. Repetir hasta convergencia
```

**Ventajas:**
- Simple y eficaz
- Funciona bien con pocos labels

**Par√°metros:**
- `kernel`: 'rbf' (Radial Basis Function)
- `gamma`: 20 (ancho del kernel)

### 5.3 Enfoque 3: Self-Training

**Algoritmo:**

```
1. Entrenar modelo con datos etiquetados
2. Predecir en datos no etiquetados
3. Agregar predicciones confiables al conjunto etiquetado
4. Reentrenar
5. Repetir hasta convergencia
```

**Ventajas:**
- Iterativo y mejora gradual
- Usa modelo base flexible

**Par√°metros:**
- `base_estimator`: DecisionTreeClassifier
- `threshold`: 0.75 (confianza m√≠nima)
- `max_iter`: 10 (iteraciones m√°ximas)

### 5.4 Comparaci√≥n Variando Labels

```python
# Para ratios [10%, 20%, 30%, 50%, 70%]:
for ratio in ratios:
    # Seleccionar aleatoriamente ratio% de datos como etiquetados
    # Entrenar Label Propagation
    # Entrenar Self-Training
    # Calcular m√©tricas
    # Comparar contra baseline
```

**Objetivo:** Encontrar punto de inflexi√≥n donde semi-supervised se acerca a supervisado.

---

## üìä FASE 6: AN√ÅLISIS E INTERPRETACI√ìN

### 6.1 Patrones Observados en Clustering

Analizar para cada cluster:
- Distribuci√≥n de ratings
- Caracter√≠sticas financieras distintivas
- Interpretaci√≥n econ√≥mica

### 6.2 Discrepancias entre Clusters y Ratings

Investigar:
- Cooperativas asignadas a cluster diferente de su rating
- Posibles razones (indicadores m√°s recientes, cambios operacionales)
- Implicaciones para pol√≠tica de cr√©dito

### 6.3 Performance del Semi-Supervised Learning

Conclusiones sobre:
- Cu√°ntos datos etiquetados son necesarios
- Cu√°l m√©todo es m√°s apropiado para esta tarea
- Viabilidad de implementaci√≥n en producci√≥n

---

## üìÅ Estructura de Salidas

```
figures/
‚îú‚îÄ‚îÄ 01_distribucion_por_rating.png
‚îú‚îÄ‚îÄ 02_matriz_correlacion.png
‚îú‚îÄ‚îÄ 03_tsne_visualization.png
‚îú‚îÄ‚îÄ 04_elbow_analysis.png
‚îú‚îÄ‚îÄ 05_clustering_results_tsne.png
‚îú‚îÄ‚îÄ 06_confusion_matrices.png
‚îî‚îÄ‚îÄ 07_semi_supervised_comparison.png

data/processed/
‚îú‚îÄ‚îÄ cooperativas_data.csv
‚îú‚îÄ‚îÄ cooperativas_clustered.csv
‚îú‚îÄ‚îÄ clustering_metrics.csv
‚îî‚îÄ‚îÄ semi_supervised_results.csv
```

---

## üîç Validaciones Implementadas

1. **Verificaci√≥n de datos:**
   - Valores en rangos v√°lidos
   - Ausencia de NaN despu√©s de limpieza
   - Duplicados removidos

2. **Validaci√≥n de clustering:**
   - k ‚â• 2 clusters
   - Todos los clusters contienen ‚â• 1 punto
   - Labels en rango [0, k-1]

3. **Validaci√≥n de semi-supervised:**
   - M√≠nimo 1 label por clase
   - Ratio de labels en [0, 1]
   - M√©tricas calculadas para todas las clases

---

## üìö Referencias Te√≥ricas

### Clustering
1. Lloyd, S. (1982). Least squares quantization in PCM
2. Rousseeuw, P. J. (1987). Silhouettes
3. Davies & Bouldin (1979). A cluster separation measure
4. Ester et al. (1996). A density-based algorithm for discovering clusters

### Semi-Supervised Learning
1. Zhou et al. (2004). Learning with Local and Global Consistency
2. Chapelle et al. (2006). Semi-Supervised Learning

### M√©tricas
1. Hubert & Arabie (1985). Comparing partitions
2. Vinh et al. (2009). Information theoretic measures for clusterings

---

## üöÄ Implementaci√≥n en Producci√≥n

Para llevar este an√°lisis a producci√≥n:

1. **Pipeline autom√°tico:**
   - Descargas mensuales de reportes
   - Procesamiento autom√°tico
   - Actualizaci√≥n de clusters

2. **Monitoreo:**
   - Estabilidad temporal de clusters
   - Drift en indicadores financieros
   - Performance de predicciones

3. **API REST:**
   - Predecir cluster/rating para nueva cooperativa
   - Consultar m√©tricas hist√≥ricas
   - Exportar reportes

---

**Fecha de elaboraci√≥n:** Noviembre 2025

**√öltima revisi√≥n:** Noviembre 2025
