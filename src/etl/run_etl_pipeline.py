"""
Pipeline ETL completo para extracci√≥n autom√°tica de datos de cooperativas.
DESCARGA autom√°tica de PDFs + EXTRACCI√ìN con OpenAI API.

Uso:
    python src/etl/run_etl_pipeline.py
"""

import os
import sys
from pathlib import Path
from typing import List, Tuple
import logging

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from etl.pdf_downloader import PDFDownloader
from etl.data_extractor import DataExtractor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_urls_from_file(filepath: str) -> List[Tuple[str, str, str]]:
    """
    Carga URLs desde archivo de texto.

    Formato esperado por l√≠nea:
    URL | Nombre Cooperativa | Rating

    Args:
        filepath: Ruta del archivo con URLs

    Returns:
        Lista de tuplas (url, nombre, rating)
    """
    urls_data = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()

            # Saltar comentarios y l√≠neas vac√≠as
            if not line or line.startswith('#'):
                continue

            # Parsear l√≠nea
            parts = [p.strip() for p in line.split('|')]

            if len(parts) >= 3:
                url, nombre, rating = parts[0], parts[1], parts[2]
                urls_data.append((url, nombre, rating))
            elif len(parts) == 1:
                # Solo URL, sin nombre ni rating
                urls_data.append((parts[0], "", ""))

    return urls_data


def run_etl_pipeline(
    urls_file: str = "data/cooperativas_urls.txt",
    output_csv: str = "data/processed/cooperativas_data.csv",
    download_dir: str = "data/raw"
):
    """
    Ejecuta el pipeline ETL completo.

    Fases:
    1. Descarga PDFs desde URLs
    2. Extrae texto de PDFs con pdfplumber
    3. Procesa texto con OpenAI API
    4. Guarda datos estructurados en CSV

    Args:
        urls_file: Archivo con URLs de PDFs
        output_csv: Archivo de salida CSV
        download_dir: Directorio para PDFs descargados
    """
    logger.info("="*70)
    logger.info("üöÄ INICIANDO PIPELINE ETL - EXTRACCI√ìN AUTOM√ÅTICA DE DATOS")
    logger.info("="*70)

    # FASE 1: Cargar URLs
    logger.info("\nüìã FASE 1: Cargando URLs...")
    urls_data = load_urls_from_file(urls_file)

    if not urls_data:
        logger.error("‚ùå No se encontraron URLs en el archivo")
        return

    logger.info(f"‚úì URLs cargadas: {len(urls_data)}")
    for url, nombre, rating in urls_data:
        logger.info(f"  ‚Ä¢ {nombre or 'Sin nombre'} (Rating: {rating or 'N/A'})")

    # FASE 2: Descargar PDFs
    logger.info("\nüì• FASE 2: Descargando PDFs...")
    downloader = PDFDownloader(output_dir=download_dir)

    urls_only = [url for url, _, _ in urls_data]
    download_results = downloader.download_pdfs_batch(urls_only)

    if not download_results['successful']:
        logger.warning("‚ö†Ô∏è  No se descargaron PDFs exitosamente")
        logger.warning("    Intentando continuar con datos de ejemplo...")
        return None

    # Mostrar resumen de descargas
    logger.info(f"‚úì PDFs descargados exitosamente: {len(download_results['successful'])}")
    if download_results['failed']:
        logger.warning(f"‚ö†Ô∏è  PDFs que fallaron: {len(download_results['failed'])}")
        for failed in download_results['failed']:
            logger.warning(f"    ‚Ä¢ {failed['url']}")

    # FASE 3: Extraer datos con OpenAI API
    logger.info("\nü§ñ FASE 3: Extrayendo datos con OpenAI API...")

    extractor = DataExtractor()

    # Preparar lista de PDFs para procesar
    # Mapear PDFs descargados con su metadata (nombre, rating)
    pdf_paths_to_process = []

    for result in download_results['successful']:
        pdf_url = result['url']
        pdf_path = result['path']

        # Buscar metadata correspondiente a esta URL
        metadata = next((item for item in urls_data if item[0] == pdf_url), None)

        if metadata:
            _, nombre, rating = metadata
            pdf_paths_to_process.append((pdf_path, nombre, rating))
        else:
            # Si no se encuentra metadata, usar valores por defecto
            pdf_paths_to_process.append((pdf_path, "", ""))

    # Procesar todos los PDFs
    df = extractor.process_batch(pdf_paths_to_process)

    if df.empty:
        logger.error("‚ùå No se extrajeron datos")
        return

    # FASE 4: Guardar datos
    logger.info("\nüíæ FASE 4: Guardando datos...")
    df_saved = extractor.save_extracted_data(output_csv)

    # Resumen final
    logger.info("\n" + "="*70)
    logger.info("‚úÖ PIPELINE ETL COMPLETADO EXITOSAMENTE")
    logger.info("="*70)
    logger.info(f"\nüìä RESUMEN:")
    logger.info(f"  ‚Ä¢ PDFs procesados: {len(df)}")
    logger.info(f"  ‚Ä¢ Variables extra√≠das: {len(df.columns)}")
    logger.info(f"  ‚Ä¢ Archivo guardado: {output_csv}")
    logger.info(f"\nüìà Distribuci√≥n de Ratings:")
    if 'rating' in df.columns:
        print(df['rating'].value_counts().to_string())
    logger.info("\n" + "="*70)

    return df


if __name__ == "__main__":
    print("\n" + "="*70)
    print("PIPELINE ETL - EXTRACCI√ìN AUTOM√ÅTICA DE DATOS DE COOPERATIVAS")
    print("="*70)

    # Ejecutar pipeline
    df = run_etl_pipeline()

    if df is not None and not df.empty:
        print("\n‚úÖ Datos extra√≠dos exitosamente")
        print(f"\nPrimeras filas:")
        print(df.head().to_string())
    else:
        print("\n‚ùå Error en el pipeline ETL")
